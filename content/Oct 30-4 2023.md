---
title: Oct 30-4 2023
---
- Test sync bit

# Todo
- [ ] composition of materialized views into derived signals
- [ ] fts issue resolution via new publish
- [ ] auth token fix via new publish
- [ ] wa-sqlite update
- [ ] rebase vfs

# Notes
possible agenda items for next time:
- authn/authz in local first dbs
- partial replication
- incremental view maintenance

Ever partial for Paul? If have more data than perms.

1. authn - bring your own token and token validator. Token is checked when reading/writing to crsql_changes via a callback.  Only checked at tx start? Or flat and checked per row?
2. authz - callback mechanism per row being inserted. Called back with primary key of row and new data? 2 phase commit so we can get whole row?
	1. Write rejection means bumping db version on the thing to send back to clients
	2. Bump col versions too? So merge just handles fixing rejection?
3. partial replication - just more streams with more cursors.
	1. Hard part is specifying what to sync.
	2. Query per table?
	3. Graph like query?
	4. Shape thing like electric?
4. IVM - mainly interested in sync rx and delta rx
	1. not so interested in actually maintaining large views in SQLite
	2. DB on top of DB tho

# Mtg
- This week bill
	- Large scope change with the need to potentially hoist to backend and sorted sources.
	- Implementing pagination
		- Virtualization difficulties. We now need to update the query to get the next window of data.
		- Count still needs it all tho. Not building a structure tho.
	- Take need... don't keep taking once we hit window size
- Take is interesting since things can fall out of the window via delete. We need a way to ask the source for more data when this happens.
	- Take is consuming all.. need to update the internals a bit to use iterables everywhere and not consume the full stream. I.e., stop after we hit N.
- Progress on after
- Replidraw? Impl the new after and take into Repliear? Rails?
	- Get perf numbers?
- Thoughts on Signia?
- Vid.. working on example set
- Async... all the complexities of an async fetch while receiving updates?


# Top
- Authn/z
- Diffi
- Native Network
- Fugue
- Tbl hash in crsql_changes? Schema check on insert?
- Reclaim some insert perf?

-> everything local is mirrored / subscribed



---

Was thinking about your row version strat and the map of all the things the server sent to the client. That approach solves a ton of hard problems (privacy changes, query changes) but I still couldn't get over how much storage it could end up being. Worst case: (all primary keys of your original dataset) * (number of users) * (avg number of devices per user).

I came up with a few ideas to reduce storage but there's only one I like.

The basic idea --

1. Keep a Cuckoo filter for the entire DB. In this filter, we place every [key, version] pair that is written, removing old versions when a new version for the same primary key is added.
2. Still keep your client view records around
3. Every so often _compact_ a client view record

Compacting a client-view record means:
1. Copying the global Cuckoo filter
2. Going through each [key, version] pair and removing it from our copy of Cuckoo filter

What we're left with, after compaction, is a Cuckoo filter which contains everything that has not been sent.

Now when we want to sync an [id, version] we can ask the filter if it is present in the set. The filter will return "definitely not in the set" or "maybe in the set." If it is definitely not in the set we can safely skip the [id, version] pair since it is "not in the not been sent set." In other words, "it is in the sent set" If it is "maybe in the set" well we send it anyway. Over-sending isn't a problem and will occur at some low likliehood.

Of course there could have been new writes that we need to sync which our copied Cuckoo filter would incorrectly say are not in the set. We handle those with the normal CVR strategy until we checkpoint / compact again.

I guess there's still a wrinkle to iron out here which is knowing which writes happened after compacting the CVR.  This seems doable though by ensuring row versions are assigned from the same monotonically increasing source. The source can be concurrent since it doesn't care if the versions are actually ever written, just so long as it doesn't give out dupes.
