{"/":{"title":"🪴 Vulcan Public Knowledge Garden","content":"\nCollaborative pieces of knowledge and notes that form the basis of the docs and posts.\n\n# [[weekly|Weekly Plans]]\n\n```dataview\nLIST FROM \"weekly\"\n```","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/0.15-release-notes":{"title":"0.15 release notes","content":"\n- clset / re-insertion\n- 2x reduction in metadata weight\n- vtab syntax\n- native ios builds\n- inbound/outbound stream tables?\n- wsocket networking layer?\n\t- pass token around in ws server and room example\n- migrator fixes\n- lookaside for primary key\n- lookaside for site id\n- lookaside for column name\n\n- sah pool? live coding of an app?\n\t- controlled lazily persisted input state component thing?\n\n\n```\ncr-sqlite v0.15.0 is around the corner.\n\n- Improved CRDT creation syntax\n- Reduced CRDT metadata by 2x\n- Native iOS builds\n- Websocket server \u0026 \"db per room\"\n- Consistent API naming\n```","lastmodified":"2023-10-12T15:43:15.223097297Z","tags":[]},"/Aug-07-11-2023":{"title":"Aug 07-11 2023","content":"\n# P1\n- [ ] _Felipe reply\n- [ ] alert on extension version mismatch in the backwards direction. e.g., db ahead of ext\n- [x] Expo incremental sync\n\t- [ ] https://github.com/expo/todo-sync-example\n- [ ] Metadata Weight\n\t- [ ] column name\n- [ ] Bootcamp task(s)\n\t- [ ] Vishal\n- [x] p2p bug\n- [ ] trigger perf explore\n- [ ] published roadmap\n- [ ] Write commit sha into build and check in test\n- [ ] Tombstone removal in client-server setups\n\t- [ ] Completely _new_ client? Don't send them any tombstones.\n- [ ] re-write db cache\n- [ ] native networking based on latest websocket server\n- [ ] schema migration!\n- [ ] Front page \u0026 demo with mobile apps...\n\n- Lazily persist via TinyBase..\n\t- pull data in from SQL from TinyBase?\n\n# P2\n- [ ] Front page demos\n- [ ] Undo/redo vite starter\n- [ ] 1 yr post\n- [ ] Fugue\n- [ ] Roadmap\n- [ ] Migrator Bugs\n\n# P3\n- [ ] Braid preso\n- [ ] Community sync\n- [ ] remove rx as arg to direct-connect-client\n\n\n# Notes\n\n## Immediate\n- vtab syntax\n- server + rest api + drop tombstones + omit tombstones to new people\n- migrator bugs\n- bootcamp tasks\n- db cache re-write + signia?\n- \n\n## Braid x SQLite\n- inspiration -- \n\t- Resurrect an old project\n\t\t- Didn't want to run server\n\t\t- Leverage compute and storage on device\n\t\t- But.. sync...\n\t- Lack of cross platform solutions here\n\t- Embeddable given client centric nature of problems related to state sync\n\t- Lack of relational solutions\n\t- Iver paper, James Long preso\n\t- yjs, braid, automerge, etc.\n- Primary use cases\n\t- Embedded\n\t\t- Most deployed software\n\t\t- Mobile devs generally use it\n\t- Edge (litestream, turso)\n- Capture reads in CRDT to understand how to order things\n\n[[sync server plans]]\n[[signia]]","lastmodified":"2023-10-12T15:43:15.223097297Z","tags":[]},"/Aug-14-18-2023":{"title":"Aug 14-18 2023","content":"\n# P1\n- [ ] nwrk\n\t- [ ] outbound stream vtab\n\t- [ ] inbound stream vtab\n- [ ] syntax\n\t- [x] CLSet vtab\n\t- [ ] CausalLog vtab\n\t- [ ] Fugue vtab? Virtual column to materialize fugue?\n- [ ] Alert on extension mismatch\n- [ ] MD Weight column name\n\t- [ ] 64bit primary key based on room client id assignment example\n- [x] Bootcamp tasks\n- [ ] SAHVFS\n- [ ] check statement cache\n- [ ] WS server w/ fs notify and litefs\n- [ ] remove use of `tables_used` component\n\n- useSync as strict mode compatible\n- Strut\n\t- ws server\n\t- useSync\n\t- strict mode?\n\t- typed sql\n\t- remove diffing in hooks? do big large vanilla fetch sttyles queries?\n\t- point and range queries not being notified properly?\n\n\nI realize I never really talk about the developer experience of having SQLite in the browser + sync.\n\nWorking on a series of 10 minute videos to cover this.\n\nPart 1 Setting up SQLite in the browser, solving persistence \u0026 cross-tab sync:\nhttps://www.youtube.com/watch?v=RtOroKmh1DE\n\n# Notes\n- rs-integration check: check for mem leaks in new clset vtab.\n- hypothesis merge many tables rs for mem valgrind\n\n- nwrk vtab\n- automigrate adjustments to replace `vtab using clset` with `update xx_schema set schema = ...`\n- [x] repo split\n- better react docs?\n- fixup cache... incorporate column level reactivity\n- SAHpool\n- typed sql in overtone test again\n- mem leak tests for error paths...\n- [x] config loading issue for ws sync worker..\n\n- improve db cache\n- maybe improve rx? at least txnality\n\t- authorizer hooks\n\t- commit hook\n\t- pre-update hook\n\nhttps://www.science.org/content/article/protein-disrupts-cells-energy-centers-may-be-culprit-chronic-fatigue-syndrome\n\n# Script\n```\npnpm create vite\npnpm add @vlcn.io/react @vlcn.io/crsqlite-wasm\nvim vite.config.ts\noptimizeDeps: {\n  exclude: [\"@vlcn.io/crsqlite-wasm\"],\n},\n\nvim main.tsx\nimport { DBProvider } from \"@vlcn.io/react\";\nimport schemaContent from \"./schemas/main.sql?raw\";\n\u003cDBProvider\n  dbname=\"mydb\"\n  schema={{\n\tname: \"main.sql\",\n\tcontent: schemaContent,\n  }}\n\u003e...\u003c/DBProvider\u003e\n\n-- update counter to use db\n\nconst ctx = useDB(dbname);\n\nconst count = (useQuery(ctx, `SELECT [count] FROM test WHERE id = 1`, [], firstPick).data || 0) as number;\n\nCREATE TABLE IF NOT EXISTS test (id PRIMARY KEY, [count]);\n\nINSERT OR IGNORE INTO test VALUES (1, 0);\n\n-- show schema migration is automagic!\n\n  ctx.db\n    .execA(`SELECT sql FROM sqlite_master WHERE name = 'test'`)\n    .then((x) =\u003e console.log(x));\n\n-- show counter sync across tabs\n\n-- update to table of items / randomwords\n\n-- https://github.com/vlcn-io/js/blob/main/packages/ws-demo/src/support/randomWords.ts\n\n\n-- million item generation\n\t-- use iid so things are ordered by time\n\n--  virtualized scroll to show the million?\n\n-- loop in sync!\n```\n\n```sql\ninsert into fee WITH RECURSIVE    cte(id) AS (       SELECT random()       UNION ALL       SELECT random()         FROM cte        LIMIT 100000  ) SELECT id, NULL as foo FROM cte;\n```\n\n```\n\"@vlcn.io/crsqlite-wasm\": \"link:../../vlcn/js/packages/crsqlite-wasm\",\n    \"@vlcn.io/react\": \"link:../../vlcn/js/packages/react\",\n```\n\n## type-sql\n```\npnpm add @vlcn.io/typed-sql @vlcn.io/typed-sql-cli\n\n\"sql-watch\": \"typed-sql -p .\",\n\nmain.sql -\u003e main.ts\n\n---\n\nimport { schema, Record } from \"@vlcn.io/typed-sql\";\n\nexport const AppSchema = schema\u003c{\n  readonly test: Readonly\u003c{\n    id: number | null;\n    count: number | null;\n  }\u003e;\n}\u003e`CREATE TABLE IF NOT EXISTS test (id INTEGER PRIMARY KEY, [count] INTEGER);`;\n\nexport type Test = Record\u003ctypeof AppSchema, \"test\"\u003e;\n\n---\n\nconst res = useQuery2(\n    ctx,\n    AppSchema.sql\u003c{ count: number }\u003e`SELECT [count] FROM test WHERE id = 1`,\n    [],\n    first\n  ).data || { count: 0 };\n```\n\n## More complicated\n\n```\npnpm add @vlcn.io/id\nCREATE TABLE IF NOT EXISTS test (id PRIMARY KEY, name TEXT);\n\ntype TestRecord = { id: string; name: string };\nconst data = useQuery\u003cTestRecord\u003e(\n    ctx,\n    \"SELECT * FROM test ORDER BY id DESC\"\n  ).data;\n\n-- mention typedsql\n\nconst addData = () =\u003e {\n    ctx.db.exec(\"INSERT INTO test (id, name) VALUES (?, ?);\", [\n      nanoid(10),\n      randomWords(wordOptions) as string,\n    ]);\n  };\n\nconst dropData = () =\u003e {\n    ctx.db.exec(\"DELETE FROM test;\");\n  };\n\n\u003cbutton onClick={addData} style={{ marginRight: \"1em\" }}\u003e\n          Add Data\n        \u003c/button\u003e\n        \u003cbutton onClick={dropData}\u003eDrop Data\u003c/button\u003e\n        \u003ctable\u003e\n          \u003cthead\u003e\n            \u003ctr\u003e\n              \u003cth\u003eID\u003c/th\u003e\n              \u003cth\u003eName\u003c/th\u003e\n            \u003c/tr\u003e\n          \u003c/thead\u003e\n          \u003ctbody\u003e\n            {data.map((row) =\u003e (\n              \u003ctr key={row.id}\u003e\n                \u003ctd\u003e{row.id}\u003c/td\u003e\n                \u003ctd\u003e\n                  \u003cEditableItem db={ctx.db} id={row.id} value={row.name} /\u003e\n                \u003c/td\u003e\n              \u003c/tr\u003e\n            ))}\n          \u003c/tbody\u003e\n        \u003c/table\u003e\n```\n\n# Bugs\n- delete all doesn't fire rx event? needs a where??\n- commit collecting on rx\n- grdb range\n- adding a column that is not null but can't be added due to existing data in automigrator should surface better errors!\n- col renamed rather than replaced.... use sql comments as hints??\n- good error if db mismatch\n\n## Tomorrow\n- Auto-migrator video\n- Sync video?\n- auth hooks, pre-update hook, db wrapper re-write","lastmodified":"2023-10-12T15:43:15.223097297Z","tags":[]},"/Aug-21-25-2023":{"title":"Aug 21-25","content":"# P1\n- [x] Y fractindex diff?\n- [x] Litefs\n\t- [ ] Now update Strut to use it via new ws-server\n- [x] RX-Cache\n- [ ] Update to latest nightly rust build\n- [ ] crsql_next_db_version --- the actual thingy... keep record of max in current tx\n\t- [ ] keep max, don't re-select on every tx. Use `data_version` to determine if re-select required.\n\t- [ ] Create new test of many connections writing then and merging?\n- [ ] Primary key compaction\n- [ ] Network https://github.com/vlcn-io/cr-sqlite/issues/38#issuecomment-1670071715\n- [ ] Reactivity Extension\n- [x] Typed SQL Bugs\n\t- [ ] TypedSQL bind types\n- [ ] Fly.io -- seq consistency?\n\t- [ ] Do not ignore `seq` if it is inserted with a changeset\n\n\n- Rooms\n- RX-Cache\n- High-perf Sync, 1 btn clk. Beefy primary? Has different run cmd for primary? E.g., to run the Java/Go process.\n\n\n---\n- Non litefs version.. Just edge nodes with volumes?\n- Forwarding changes to one another for durability or edge access?\n\nStill have needs of strong consistency tho.\n\nLiteFS questions:\n- Different primaries for different DBs? Rather than one primary managing all writes for _all_ dbs?\n- Different \"app\" to act as primary altogether. E.g., beefy highly concurrent Java app.\n- Wrapping LiteFS to provide 1 click deploys w/ cr-sqlite correctly wired up. \"Press a button, get a litefs + crsqlite setup\" Partykit setup?\n\n# Notes\n1. as_ordered seems bugged in the latest strut port.\n\t1. Write some py tests to figure this out\n2. Our DB cache likely has some correctness problems. Maybe masked by the fact that cached values are immediately removed after query completion? So it is just coalescing duplicate queries dispatched at same time.\n\t1. rx-cache will fix this\n\t2. rx-cache against ahp against synchronous opfs should be... big wins\n3. use point and range queries broken but we can fix via rx-cache\n4. Roy's build for rx-cache might not be the best given his sync build still uses async on step?\n\t1. Maybe best starter given that's where we're at and shouldn't matter to rx-cache since it'll be internal to that api\n5. create virtual table syntax for ordered columns\n\nStill remaining:\n- Causal Log\n- Counter\n- Fugue\n\n\nPay...\n\n---\n\nBG:\n- R\u0026D LM Collab\n- Deletion, Data collection, DYI, Secure DBs, Case Management, Inv Tooling, Privcy\n\t- EntSchema, EntIntegrity, EntLambda\n\nMy focus:\nOn device.\nEmbedded DBs don't solve the needs of app developers today.\n\n- Collab / Multiplayer\n- Latency\n- Offline edits\n\nWhen SQLite came out, users generally had a single device.\n2010 - many device\n2020 - many device + multiplayer/collab are table stakes, reactive patterns too\n\nWhy can our DB not solve these for us?\n- Rx\n- Sync (multiplayer / collab / offline support)\n- Sharing / RLS\n\nWhy cr-sqlite?\nLooked at edge and:\n\t- Edge still doesn't solve network hiccups, offline and multiplayer problems\n\t- Cost is still a factor with edge\n\t- More complex now that you're... replicating part of your dataset?\n\t\t- Getting data to device problem still exists\n\nOrig plan: Postgres/mysql/whatev integration\nWhy looking at D1, Litefs, Turso and things now?\n- Currently compatible with SQLite, quickest way to \"go to market\" \n- Hinges on / unanswered:\n\t- Use cases for \"db per doc\"? \"db per user\"?\n\t- \"internet of files\"\n\nNext up:\n- Rich text (docs)\n- Causal Log (chat like, comment stream)\n- Counter\n- rx-cache\n- postgres?\n\n","lastmodified":"2023-10-12T15:43:15.223097297Z","tags":[]},"/Aug-28-1-2023":{"title":"Aug 28-1","content":"[[Aug 21-25 2023|prev]]\n# P1\n\n- [ ] Compact primary keys\n- [ ] litefs\n- [ ] rx\n- [ ] causal log\n- [ ] sequence\n- [ ] optimized db version tracking\n\n---\n# Notes\n","lastmodified":"2023-10-12T15:43:15.223097297Z","tags":["weekly"]},"/Beyond-CRDTs-The-Next-Frontier-of-Local-First":{"title":"Beyond CRDTs, the next frontier of local-first","content":"\n#presentation\n\n[[lofi presentations]]\n\nA major problem, and the one receiving the most attention, in collaborative \u0026 local-first software is figuring out how to merge concurrent edits between users. We're going to assume that this is completely solved and that the developer experience around it is great.\n\nIn this world:\n\n- CRDTs can live alongside, and be joined with, normal data\n- Precise merge semantics is easy to define and user intent is easy to preserve by composing off the shelf CRDTs\n- Locally stored CRDTs can be easily queried over and indexed through something like SQL\n- CRDTs can easily be paged out to disk and loaded to memory as needed\n\nThings look bright. If we go off to try to build a local-first application in this world, however, we get stuck fast.\n\nHistory lesson on _how we got here_\n\n# The Next Frontier\n\nWe get stuck on a new frontier of problems:\n- Auth\n- Schema evolution\n- Heterogenous devices\n\t- Get data where needed, when needed\n\t- Why this is a problem\n\t- Cloud history and how we got here\n\t- Why we love cloud for this\n\t- https://rocksdb.org/blog/2022/11/09/time-aware-tiered-storage.html\n- Multi-tenancy\n- Permissions \u0026 Sharing\n- Data Flow\n- Notifications?\n- Server Authoritative data... Or centralized decisions.\n\nTo understand the most pressing issues blocking development, we're going to be taking the mindset of a scrappy startup trying to ship a local-first app _today_. Given that, we'll make some simplifying architectural decisions and see which of the above problems still remain.\n\n# Auth\n\nThe simplest way to handle auth today would be to force all access to data that exists off-device to go through a central service. The client can prove their identity to the server, the server ensures only authorized writes are synced to other clients.\n\nAuth is a non-issue under this constraint.\n\n## Schema Evolution\n\nAllowing clients to make writes while offline does introduce some problems from a schema evolution perspective. We can render this problem a non-issue by:\n\n1. Leveraging the fact that all changes must go through a central server\n2. Disallowing sync between client and server while they have mismatched schema versions\n\nClients must upgrade to the current schema version (either via page refresh or installing an app update) to continue syncing their changes.\n\n## Heterogenous Devices\n\nThis is our first _real_ problem. The issue here is that users of our app will have many different devices on which they'll install our app. Each device will have different storage and compute characteristics. All the data that can fit locally on the user's laptop likely will not fit on their phone or watch or whatever.\n\nSo we're faced with the problem of how to define a subset of the user's data to sync to a device.\n\nHow is this solved in traditional apps?\n\n```ts\nfunction Component() {\n\tconst { isLoading, error, data } = useQuery('repoData', () =\u003e\n    fetch('https://api.github.com/repos/tannerlinsley/react-query').then(res =\u003e\n      res.json()\n    )\n  )\n}\n```\n\n\n---\n\nTo make these problems tractable, we'll assume that the local-first apps are using a server to broker all communication between clients.\n\nGiven the use of a central server, auth becomes a non-issue.\n\n\n---\n\n\n## Schema Evolution\n\nSchema Evolution is interesting but we can take \n\nOut of these 5 problems, we find that the first two are pretty easily solved by making some pragmatic choices.\n\n- Force sync to go through a central server\n- Disallow a client to sync if client_schema_version != server_schema_version\n\nThe other three seem impenetrable. What makes them interesting and alluring, however, is that they're all variations on the same problem.\n\n# Three in One\n\n**Heterogenous devices:**\n\n\u003e A single user has many devices on which they'll install the same local-first application. Each device has different compute and storage characteristics.\n\nThis is a problem since what we can store on a phone != what we can store on a laptop. The capacity of what a single user can store will differ based on which device they're using to interact with the app. We can't assume all data generated by the user on their laptop can ever be fully synced to their phone.\n\nThe problem then is knowing what subset of the user's data should be synced to a given device.\n\n**Multi-tenancy:**\n\n\u003e Saving all data for all users in a single database on the backend. E.g., traditional application architectures on a multi-tenant DB like Postgres.\n\nMulti-tenancy is implemented in client-server models to facilitate features around sharing. Data produced by one user might need to be visible to other users. The easiest way to handle that is to stick everything into a single database.\n\nThe problem this poses for local-first, however, is in understanding what data to sync to which users.\n\n**Permissions \u0026 Sharing:**\n\n\u003e Giving users access to new pieces of data.\n\nThis is similar to multi-tenancy in that permissions would be used as the mechanism to partition tenants within a database. Thinking of it as permissions, however, brings to light the dynamic nature of the problem.\n\nA new user may install a local-first app. At first, they only have access to their own data but eventually they're added to different roles or user groups. Now they suddenly have access to millions of rows of data produced by other users.\n\nHow do we detect a permission change has occurred and what things are newly visible? How do we know which of the newly visible items should be synced?\n\nAll of these problems are about having more data in one place than can fit, or be accessed, in another.\n\nThe ever-present `memory -\u003e disk -\u003e network` data tiering still exists, even in Local-First software.\n\nHow do we attack this problem?\n\n^-- treat problems individually. heterog / bottomless is unqiue as it necessitates the network bound. Could create a new problem related to devx of getting data async.\n\n# Developer Experience\n\nAll of this inevitably bubbles up into the developer experience. The developer needs to know whether a query is immediately resolved from in-memory data, is waiting on data from disk, is waiting on data from the network.\n\nWaiting on data from the network...\n\nIgnoring the problem and pretending all data is \"always available\"\n\nAll of these issues bubble up into the developer experience of writing local-first software. The tiers of data:\n\n`memory -\u003e disk -\u003e network` clearly still exist, even in a local-first stack.\n\n---\n\nis the fact that a single user has many devices, each of which has different compute characteristics and each of which should be able to run \n\n\n\n\n\nOr maybe:\n\"Sorry, local-first isn't easier\"\n\n---\n\nCRDTs (or some other convergence mechanism) is only the first frontier.\n\nWhat follows:\n1. Permissions\n3. Bottomless replication due to size constraints\n4. Moving out of memory and async vs sync browser quirkiness\n5. Multi-tenant DBs\n6. Query based sync\n7. Perf? The \"functional relational\" is a separate concern from local-first...\n8. Server authoritative?\n\nWhat follows re-cast:\n\t- Bottomless replication (change devices examples)\n\t- Auth\n\t- Sharing (variation on multi-tenancy of many users in 1 db)\n\t- Better RX?\n\t- Local networking?\n\t- App delivery? (dweb tomorrow example)\n\t- Intent?\n\t- what is network delayed vs what is immediate? Is the immediate data really complete? Mixing consistency models\n\t- Local-first may not actually simplify development at all. It just looks simple at the outset.\n\t- The sync but not sync issue. UI -\u003e Disk -\u003e Network.\n\nSome good thoughts in this vein are forming in [[row level security]], [[client defined query based sync]], [[incrementalism]], [[bloom filters for row level security]], [[inverted database]]","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/DB-cache":{"title":"DB cache","content":"\n#project\n\nThe JS integration currently makes use of a DB Cache. This cache only serves to collapse duplicate queries, made in the same event loop tick, into a single query. Any other data caching is done within the reactivity layer (currently the `useQuery` hook. see [[react integration]])\n\nWe could be smarter and cache here indefinitely / until invalidate. Why do this? Well because many components might be issuing the exact same query and the `useQuery` cache is not shared between components.\n\nA smarter DB Cache also can be a resting place for [[fully synchronous reactivity]].","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/Incremental-View-Maintenance":{"title":"Incremental View Maintenance","content":"\n#project\n\nMost apps are read, not write, heavy. Incremental view maintenance lets us shift work to deal with this reality. We can do _more work_ on write in order to _save time_ on read.\n\nThe Noria project shows it best:\n\n![[/attachments/Pasted image 20230628100342.png]]\n\u003e Noria automatically keeps cached results up-to-date as the underlying data, stored in persistent _base tables_, change. Noria uses partially-stateful data-flow to reduce memory overhead, and supports dynamic, runtime data-flow and query change.\n\n# Implementing\n- First we need to map writes to impacted queries / views via something like an [[inverted database]].\n- Potentially we would do [[differential data flow]] against the new write for the given query / view\n\n[[streaming reactivity]]","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/IndexedDB-slow-transactions":{"title":"IndexedDB slow transactions","content":"\nhttps://github.com/rhashimoto/wa-sqlite/discussions/86","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/July-03-07-2023":{"title":"July 03-07, 2023","content":"[[prioritization legend]]\n# P1\n- Fly.io\n\t- [x] Finish prepared statement caching to [improve vtab perf](https://github.com/vlcn-io/cr-sqlite/issues/252)\n\t- [ ] Finish re-insertion\n\t\t- [ ] Version comparison must now take into account causal length\n\t- [x] Backup and restore thoughts?\n\t- [x] Speed up `SELECT DISTINCT(db_version) FROM crsql_changes WHERE site_id IS ? AND db_version \u003e= ? AND db_version \u003c= ?;`? https://www.sqlite.org/c3ref/vtab_distinct.html , https://github.com/vlcn-io/cr-sqlite/tree/distinct\n\t\t- Seems currently untenable without weird hacks. Just push them to use clock tables directly or : https://discord.com/channels/989870439897653248/989870440585494530/1126484059871264768\n\t- [x] Benchmark insertions against tables with millions of rows\n\t\t- https://discord.com/channels/989870439897653248/989870440585494530/1126462170343944242\n- https://2023.splashcon.org/\n\t- **talk submission?** Due next wed.\n- Webkit\n\t- File bug report for [this](https://github.com/rhashimoto/wa-sqlite/discussions/94#discussioncomment-6316242)\n- Vision Casting\n\t- [ ] \"What if this was your world?\" - live coding a collaborative app from the vite-starter\n\t\t- Automigrate resilience\n- Automigrator Stability!\n- server authoritative!\n- rm that `count(*)` in crsql changes vtab\n- vtab create table syntax\n- document: NO AUTO-INCREMENT!\n- Riffle\n\t- [x] Lazy RX interval refresh\n\t- [ ] IVM for select + join / inverted DB\n\t- [ ] TreeQL [[type safe sql]]\n\t- [ ] Caching w/ inverted db consult\n\t- [ ] Theoretical best case perf via shared access pool\n# P2\n- Turso\n\t- [ ] e2e integration example\n- TinyBase\n\t- [x] Demo app with tinybase\n\t\t- [x] Not ready enough to demo, see [[tinybase]]\n\t- [x] Review integration\n\t\t- [[tinybase]]\n- Framer\n\t- [x] JSON hierarchies\n\t- [x] Property table\n\n# P3\n- Presentation Prep\n\t- [[Beyond CRDTs, The Next Frontier of Local-First]]\n\n# Random Notes\n- Better testing of pack/unpack loop\n- **Front page**\n- **Vision casting**\n- What's next after data sync?\n- pre-seed\n- tinybase\n- **auto-migrate**\n- treesql\n- overtone perf\n- theoretical speed limits of SQLite in browser\n- row level security\n- inverted db and multi-tenancy\n- varint js binary serializer\n- public roadmap publication\n\nWould like to start the Rust port... Simplify build, contribution, testing, ...\nTreeQL for what? Better React integration.\nWebKit bug tho..\n  Make a minimal wa-sqlite project that loads the db and auto-execs enough queries to kick off issues.\n  Try O2 instead of Oz?\nWA-SQLite theoretical limits... / benchmarks for our application load.\n\nDoes DB version need to be set to `max(self, peer)`? Only col versions need that bumping.\n\nNeed column specific triggers so we can detect a modification of primary key and issue corresponding deletes in that case.\n\n# Thoughts of the week\n[[type safe sql]]\n[[TreeQL]]\n[[Beyond CRDTs, The Next Frontier of Local-First]]","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/July-17-21-2023":{"title":"July 17-21, 2023","content":"\n\n- [x] XCode \u0026 SQLite static\n\t- [ ] Other SQLite iOS thoughts\n\t\t- [ ] https://medium.com/capital-one-tech/experimenting-with-sqlite-in-ios-ae9dec92dba\n\t- [ ] Refs:\n\t\t- [ ] https://github.com/swiftlyfalling/SQLiteLib\n\t\t- [ ] https://stackoverflow.com/questions/823706/compiling-custom-sqlite-for-an-iphone-app\n\t\t- [ ] https://github.com/groue/GRDB.swift/blob/master/Documentation/CustomSQLiteBuilds.md\n- [ ] libsql close hook - https://github.com/libsql/libsql/issues/62#event-9871201225\n- [ ] sqld notifications\n- [ ] commit sha in cr-sqlite\n- [ ] More tests in Rust to replace deleted c tests ^97e528\n\t- [ ] Many tables merging\n\t- [ ] Order by consumed\n\t- [ ] Filter perf, index selection, explain query plan\n\t- [ ] hypothesis rust?\n\t- [ ] sqlite memory tests in python tests? valgrind python? o_O even reasonable?\n\t- [ ] explicit correctness case tests (lamport condition, tied versions, tied values, ...)\n\t- [ ] only one sentinel on row create even with many pks\n\t- [ ] pk adjustment stuff work if the user is doing aliased rowids for their pk?\n\t- [ ] pk adjustment junction edges. pkonly table.\n- [ ] re-insertion\n- [x] col level trigger, pk adjustment\n- [ ] vtab syntax\n- [ ] vite-express instead of fork\n- [ ] less copies on reading unpacked data\n- [ ] prepare query against changes... LRU cache? Capped cache?\n- [x] typed sql split\n- [ ] IVM\n\t- [ ] Aphrodite\n\t- [ ] TypedSQL\n\t- [ ] R-Tree\n- [ ] Frontpage Guide\n\t- [ ] Automigrator\n- [x] [[Beyond CRDTs, The Next Frontier of Local-First]] finalize\n- [x] Rust Perf vs C Perf\n\t- [ ] note that rust conversion is _slightly_ slower in the changes vtab.\n\t\t- could be `select 1 limit 1` for delete?\n\t\t- could be btree?\n\t\t- could be non static methods?\n\t\t- could remove missing column compare\n- [ ] Newletter..\n- [ ] Allocation reduction, wasm boundary speedup\n\t- [ ] https://github.com/SkipLabs/skdb/blob/main/packages/skdb/src/skdb-browser.ts\n- [ ] typed-sql intellisense\n\t- [ ] https://github.com/segmentio/ts-mysql-plugin\n\nWhat actually happened:\n- expo support\n- win issue debug\n- rust conversion\n\n# Random Notes\n- rust final\n\t- cross compile final\n\t- prebuilts\n\t\t- droid builds\n\t\t- dylib ios\n\t\t- static ios\n\t\t- swift custom bindings GRDB\n- cl final\n- roadmap final\n\n```\n[!] The `ios-starter [Debug]` target overrides the `LD_RUNPATH_SEARCH_PATHS` build setting defined in `Pods/Target Support Files/Pods-ios-starter/Pods-ios-starter.debug.xcconfig'. This can lead to problems with the CocoaPods installation\n    - Use the `$(inherited)` flag, or\n    - Remove the build settings from the target.\n\n[!] The `ios-starter [Release]` target overrides the `LD_RUNPATH_SEARCH_PATHS` build setting defined in `Pods/Target Support Files/Pods-ios-starter/Pods-ios-starter.release.xcconfig'. This can lead to problems with the CocoaPods installation\n    - Use the `$(inherited)` flag, or\n    - Remove the build settings from the target.\n```\n\n\nNeed:\n- universal sim\n- universal not sim\n- universal macos\n\nShould we collect into single rows? Packed rows from crsql changes? Option to toggle on whole row sync? Take the minmax db version of the row.\n\n- custom vtab for full row based replication?\n- split up typed-sql\n- https://cocoapods.org/pods/sqlite3\n\n\nhttps://github.com/stephencelis/SQLite.swift\nhttps://theswiftdev.com/how-to-call-c-code-from-swift/\nhttps://stackoverflow.com/questions/75056984/building-a-static-library-with-go-build-for-iphone-simulator\nhttps://stackoverflow.com/questions/71036701/build-a-single-a-for-simulator-and-device-containing-arm64-targeting-m1-simula\n\n\n```\n\"end\": \"(?\u003c=(`|\u003e\\\\s*[^\\\\s`]))\",\n```\n\n\nDoc size drawback on sharing?\nIncrementalism on sharing?\n\nhttps://liuxin.design/_portfolio/case-management-system-user-interface/\n\n`bind params prepared statement insert types` -- ","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/July-24-28-2023":{"title":"July 24-28, 2023","content":"\n- Re-insert\n- Compressed metadata\n- VTab syntax\n- Fugue\n\n- [ ] Expo bindings, prepare statement\n- [ ] XCode SQLite docs\n- [ ] Frontpage tutorials\n- [ ] Re-insertion completion\n- [ ] IVM\n- [ ] Automigrator robustness\n- [ ] More Rust tests to replace C tests\n\t- [ ] see [[July 17-21, 2023#^97e528]]\n\t- [ ] filter\n\t- [ ] causal length invariants\n\t- [ ] migration\n\t- [ ] method by method?\n- [ ] Compressed clock table format?\n\t- [ ] Single row w/ all metadata?\n\t- [ ] [pk, col_clocks, last_db_version, db_versions, last_site_id]\n\t\t- [ ] really just fixes: pk redundancy, col name redundancy, site_id redundancy\n\t\t- [ ] col_names can go to ints, site_id can go to lookup table for first pass compress\n\t- [ ] VTab creation\n\t- [ ] Track col ids through alters so we don't re-use a number that has clock entries for a different column\n\ndoes_cid_win needs causal length value... since we must check that then check row clocks.\nfetching from vtab must return causal length value... so we can have it at insert time\n\nSAH Pool --\nhttps://sqlite.org/forum/forumpost/dc2f39cd5cf1bfc4\n\nhttps://sqlite.org/forum/forumpost/9597d8d9f3","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/July-31-04-2023":{"title":"July 31-04 2023","content":"\n# P1\n- [ ] Expo demo incremental sync fix\n- [x] Reply to James on hooks\n- [x] Reply about hrly rate\n- [x] Finish re-insertion 😬\n- [ ] Finish vtab syntax\n- [ ] Finish migrator bugs\n- [x] Cut metadata weight\n- [x] Resolve TypedSQL bugs in Overtone (overlap with other sql tags, mainly)\n- [ ] Add undo/redo to vite-starter\n- [ ] presence and temp tables...\n- [ ] sah pool migration\n\t- [ ] no asyncify?\n- [x] update wa-sqlite\n- [ ] zip delivery of extensions to eliminate rename\n- [x] null row record means causal length of 1\n- [ ] Incremental merge for Expo demo...\n\n# P2\n- [ ] Fugue?\n- [ ] Front page story?\n- [ ] 1 year post?\n- [ ] Publish roadmap?\n\n# P3\n- [ ] Braid preso?\n- [ ] Community sync\n\ntalk to https://twitter.com/artman ?\n\n\n# Notes\n- review https://github.com/expo/expo/pull/23791\n- compression to reduce insertion time? more insertion testing to figure out where non-linearity comes from?\n- Android native library -- https://blog.arik.io/compiling-c-code-as-android-shared-libraries-6924803c00db\n\n- Use re-insertion in Strut\n\t- Use typed-sql there too then\n\t- fixup the db cache as well then?\n\t- update the vite starter to include re-insertion?\n\n- Swap to a deno starter? https://deno.land/manual@v1.35.3/node/how_to_with_npm/vue\n\n## Compression\n\nOther ideas...\n\nAll in a single row? What db version would this row be tagged with?\nThe largest version of the latest change?\nThen what cells should be pulled from it?\nAlways all cells since we don't know the other cell versions..\n\nOk for querying tho...\nWe can pack all db versions but leave 1 unpacked for querying.\n\nWould change delivery time of cells by quite a bit to be bundled in further out transactions.\n\n\n- cid from table_info, grabbed in returned order, in memory\n\t- migrations create temp table to re-number metadata if col names get new numbers\n- site_id in own table\n- pks... last frontier\n\n\nSync whole row on every change.... They can manually normalize if they need finer grained sync.\n- Row would have db version, not cell.\n- Cells have col versions still tho however.\n- Can get rid of default value or null requirement(s)\n\nCan we do col level sync while doing row groups?\n\nFor in-order merge yes. Do 2-phase commit. Gather rows together and insert at once.\n\n\n```sql\nSELECT COALESCE(\n\t(SELECT __crsql_col_version FROM \"hoot__crsql_clock\" WHERE \"b\" IS 1 AND __crsql_col_name = '-1'),\n\t(SELECT 1 FROM \"hoot__crsql_clock\" WHERE \"b\" IS 1)\n)\n```\n\n\n@seed(98817124127868963542241055112675129876)\n\n\n```\nThe deals I have with Expo.dev and Fly are a bit simpler.\n\nIn that they are both driving the integrations and having me consult / review diffs / add missing features to cr-sqlite.\n\nThis works well since it isn't a huge mental tax trying to get familiar with their products, how cr-sqlite should be integrated, how to integrate it, etc.\n\nWith turso it's a bit reversed. My driving an integration into your product. So I think th\n```\n\n\ntiming:\n```\ninsert into fee WITH RECURSIVE    cte(id) AS (       SELECT random()       UNION ALL       SELECT random()         FROM cte        LIMIT 100000  ) SELECT id, NULL as foo FROM cte;\n```\n\nBest way to handle row level security?\nWhole db sync with filter scales to some level.\n\nQuery based sync is the other option but how to query based sync in an incremental way?\n\nThis is a good start: Parametrized query per table? Joining clock tables where db_version \u003e x?\n\nArbitrary queries that get col version information tacked onto return results? So they are mergable?","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/June-26-30-2023":{"title":"June 26-30, 2023","content":"\n[[prioritization legend]]\n\n# P1\n- [Fly.io](https://fly.io)\n\t- [x] Cache prepared statements for virtual table insert and read to [improve vtab perf](https://github.com/vlcn-io/cr-sqlite/issues/252)\n\t- [ ] Enable [re-insertion](https://github.com/vlcn-io/cr-sqlite/issues/71)\n\t- [ ] Backup and restore? [gh-145](https://github.com/vlcn-io/cr-sqlite/issues/145#issuecomment-1604695054)\n- Wekbit\n\t- [ ] File bug report related to:\n\t\t- https://github.com/rhashimoto/wa-sqlite/discussions/94#discussioncomment-6316242\n- Automigrate\n\t- [ ] Vite starter automigrate \u0026 further testing\n---\n# P2\n- Overtone / [Riffle.systems](https://riffle.systems)\n\t- [ ] First pass [[Incremental View Maintenance]] for SELECT + JOIN\n\t- [ ] Lazy reactivity for things like the import service\n- Rich Text\n\t- [ ] Diamond Types integration POC, [potentially Fugue as well](https://github.com/vlcn-io/cr-sqlite/issues/65)\n- [Turso.tech](https://turso.tech)\n\t- [x] Turso cr-sqlite and sync to/from the browser over http\n\t\t- `turso db create --canary --enable-extensions`\n\t\t- `turso db shell crack-pestilence`\n---\n# P3\n- [TinyBase.org](https://tinybase.org)\n\t- [ ] Review integration\n\t\t- Potentially we should do some batching to ensure we're doing all reads in a single transaction due to [[IndexedDB slow transactions]]. I.e., we can only do ~100 IDB transactions per second but ~100,000 reads per second within a single IDB transaction. We've done this with the stock [[react integration]].\n\t\t- How is a TinyBase model hydrated to stat? Can we bind a model to a query?\n\t\t- Just need to overall understand tinybase better and try to build something with it\n- [Framer.com](https://framer.com)\n\t- [ ] POC for [[converting relations to hierarchies]]\n\t- [ ] POC for \"property table\" to model \"arbitrary JSON\"\n- Pre-seed\n\t- [ ] GTM\n\t- [ ] 1 year roadmap\n\t- [ ] Terms\n\t- [ ] Investor list\n\n# Random notes\n- Collabs\n\t- https://collabs.readthedocs.io/en/latest/\n- Vision for local first\n\t- Bottomless\n\t- RLS\n\t- Multi-tenancy\n\t- Functional Reactive, Async -\u003e Sync\n- Are we clearing bindings in addition to resetting?\n\t- Does SQLite free memory if binding is not cleared but is overridden?\n- Test that binding the same sqlite_value to many statement doesn't err\n- update hook and transactions\n- unquoted values and impact on js network layers:\n\t- byte arrays / blobs\n\t- bigints\n- front page demos / intro\n\t- webkit fix\n- Vision casting...\n\n```\nall_scripts = (3, [[], [(0, '2732596e-7fd0-492e-8f26-fd85538d120d', (None, None, None, None, None), None)], []], 1)\n```","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/Oct-2-6-2023":{"title":"Oct 2-6 2023","content":"# P1\n- [ ] CID for compact\n\t- [ ] Rowid fixup\n- [ ] PKs compact extra test for expected results\n- [x] LSM tree for materialite\n- [ ] SQLite walls blog?\n\n# P2\n- [x] Windows build?\n\n\n---\nJob openings are, unfortunately, a bad thing since the next generation is smaller than the retiring boomers.\n\nInterest rates are going to stay high till 2050 imo. Unless we get mass waves of legal immigration.\n\n\n---\n\nBlog","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/Oct-9-13-2023":{"title":"Oct 9-13 2023","content":"# P1\n- [ ] CID for compact\n\t- [ ] rowid fixup?\n- [ ] PKs compact against expected results?\n\n\n# Writing\n- [ ] Reactivity from the main thread\n\t- [ ] 1. Point updates\n\t- [ ] 2. Updates optimistically when join or where column untouched by mutation\n\t\t- [ ] Map from row -\u003e queries using row and project into them\n\t- [ ] 3. Partially materialized differential dataflow pipeline\n\t- [ ] 4. Mapping write to impacted queries by indexing queries\n\t- [ ] 5. In-memory SQLite of data in use by app, full db in worker. Same as partial replication from server to client.\n- [ ] Row level security\n\t- [ ] 1. Callback\n\t- [ ] 2. Bloom filter of _what has not been sent_ or _what has failed privacy_\n\t\t- [ ] thing can now be seen and _definite failed pirvacy_, (bc maybe in not sent set) send it nomatter db version. \"failed privacy set\" -\u003e \"definitely did not fail privacy\" \u0026 \"maybe failed privacy\"\n- [ ] Partial sync\n\t- [ ] If query shape changes, full re-sync?\n- [ ] Subscription from the backend\n\t- [ ] Each active session has a set of active partially hydrated differential dataflow pipelines\n- [ ] Partially hydrated differential dataflow\n\t- [ ] Hydrated just from initial query results\n\t\t- [ ] How to know when to pull new?\n\t- [ ] Join index is against actual DB tables not in-memory pipeline index\n\t- [ ] ","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/Sep-11-15-2023":{"title":"Sep 11-15 2023","content":"# P1\n- [x] Video\n\t- [x] SQLite as a collaborative application file format\n\t- [x] Strut staring it off\n\t- [x] Tab sync\n\t- [x] CRR\n\t- [x] Local-only (undo, selection state)\n\t- [x] Sort\n\t- [x] Constant events / metadata size\n\t- [x] Schema change \u0026 reload \u0026 auto-migrate\n\t\t- [x] Table is sensitive to schema?\n\t- [x] Switch to WebSocket streaming at the end\n\t- [x] Maybe even introduce typed-sql?\n\t- [ ] full text search in the example too?\n- [ ] Metadata Weight\n\t- [ ] PKs\n\t\t- [ ] new table\n\t\t- [ ] trigger updates\n\t\t- [ ] vtab select\n\t\t- [ ] vtab insert\n\t\t- [ ] migration / backfill stuff?\n- [ ] Manual Migration\n\t- [ ] Disable auto-mig\n\t- [ ] Provide path to specify migration files\n- [ ] Personal\n\t- [ ] Car\n\t- [ ] Rental\n\t- [ ] Projector\n- [ ] Blog abt \"this time it is different\"?\n\t- [ ] https://news.ycombinator.com/item?id=37488034\n\t- [ ] \"Local First Misconceptions\"\n- [ ] wa-sqlite sqlite 3.43\n# P2\n- [ ] Live Resolvers -- https://github.com/captbaritone/redux-to-relay-with-live-resolvers-example/fork\n\t- [ ] Past: https://github.com/tslater/reactnative-relay-offline\n\t- [ ] https://github.com/captbaritone/redux-to-relay-with-live-resolvers-example/tree/SQL-2\n\nAutomigrate errors still exist?\nStatus code 500 not showing error on user interface?\n\n\n- https://twitter.com/smashingmag/status/410118699577012224\n- strut.io\n- auto-prepare insert statements? and re-use?\n\n\n# Notes\n\nThoughts:\n\"Application file format\"\n- Postgres backend\n- SQLite 2 SQLite\n- P2P webrtc\n- Reactivity / instant interaction\n- Text CRDTs?\n- Rebase and mutation(s) approach\n- Whole db? partial? row level securit?\n\n\n- [x] crsql_create_crr needs to update TableInfo in ext data such that we do not need to call `ensureUpToDate`\n- [ ] Data not replicated after primary key change? Clock records not moved or whatever?\n- [ ] auto-migrator fails with trigger changes?\n\t- [ ] vite starter better refresh by moving root ou. p2p repro fails refresh\n\t- [ ] Drop tirggers if triggers taken out of schema?\n\nUpdate table info after migrate. Why? Because we use it for merge to construct pk where lists and the like.\n\nWe need to invalidate cached statements for a given table post migrate...\nWe could also not use the b-tree and instead cache statements in table info as needed.\n\n---\n\nWe can statically analyze and pull fragments used by components and combine them into an incremental pipeline and incrementally maintain the view which is the UI.\n\n\n---\n\nNew trigger:\n```\non_insert() {\n  let key = SELECT key FROM lookaside WHERE pk_where_list;\n  if !key {\n\t// could reverse and do inser or ignore since insert common\n\t// case is it not existing\n    key = INSERT INTO lookaside VALUES (...) RETURNING key;\n    // non-conflict clock set\n  } else {\n    // conflict clock set\n  }\n}\n\non_update() {\n  let key = SELECT key FROM lookaside WHERE pk_where_list;\n}\n\non_delete() {\n\n}\n\n// prepared:\n// 1. SELECT key FROM lookaside\n// 2. INSERT OR IGNORE INTO lookaside VALUES () RETURNING key;\n// 3. \n// Causal length can move to lookaside too!\n// Hmm.. maybe later since we need sentinel log.\n```\n\n---\n\nSQLite was the perfect embedded database for the last two decades but it's lacking in today's era of collaborative and multiplayer applications.\n\n  \n\nTo remedy that, I've extended SQLite over the last year to give it the ability to support multiplayer and collaborative applications. It works by allowing SQLite databases, on different devices, to be merged together in real time.\n\n  \n\nThanks to help and feedback from https://fly.io, https://expo.dev and https://braid.org I've put together an end to end developer experience around SQLite state synchronization to help anyone build the next Figma, Google Docs or Linear.app without needing to be a distributed systems expert.\n\n  \n\nWatch the video overview here:\n\nhttps://www.youtube.com/watch?v=T1ES9x8DKR4\n\n  \n\nOr check out the project website: https://vlcn.io/docs\n\n\n```\n-Wl,--no-as-needed -ldl\n-pthread -lpthread\n```\n```c\nvalgrind --leak-check=full \\\n         --show-leak-kinds=all \\\n         --track-origins=yes \\\n         --verbose \\\n         --log-file=valgrind-out.txt \\\n         ./executable exampleParam1\n```\n\n\n**#sqlite** **#crdts** **#collaboration**","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/Sep-18-22-2023":{"title":"Sep 18-22 2023","content":"# P1\n- [ ] Feedback https://expo.notion.site/CRSqlite-REST-syncing-example-69b0d4423d5547c98f24b80fe2323071\n- [ ] Metadata weight pk finalize\n- [ ] JS has cr-sqlite as sub-repo\n- [ ] RX Demo\n\n- [ ] update wa-sqlite version\n- [ ] db version as col version\nEach `col_version` is currently incremented independently. We can instead set it to the current `db_version` to ensure that all values set in the same transaction can also all win together when merging.\n\nThis isn't guaranteed since the peer being merged into could be way ahead in db_version overall but have some records behind in db_version.","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/Sep-25-29-2023":{"title":"Sep 25-29 2023","content":"# P1\n- [ ] Turso\n\t- [x] Close hook doesn't exists for loadable extensions?\n\t- [ ] fails to compile integration check for turso? Just move on to latest commits with newer integration check?\n- [ ] cr-sqlite\n\t- [ ] compress out column names\n\t- [ ] Server compensations and examples\n\t- [ ] Native Network\n\t- [ ] db-version update as col-version\n\t- [ ] WebSocket permissions example\n\t- [ ] Text, Counter, Causal Log\n\t- [ ] DB Version for Col Version\n\t\t- [ ] How to do server compensations and invariant preservations\n\t\t\t- [ ] vtab triggers? 2phase commit? Run after all write of the block are done?\n\t\t- [ ] Full tx or no tx in server centric setup?\n- [ ] Website\n\t- [ ] Schedule meeting link\n\t- [ ] Promotional copy on each segment reactive/relational/sync/etc.\n- [x] NextJS starter with useFormHook?\n\t- [x] doesn't work\n- [ ] auto-prepare mutations and queries for user?\n- [ ] multi-hook\n\t- [ ] duckdb subs? oltp pref?\n- [ ] Optimistic Tables rather than CRRs\n\t- [ ] hold db version\n\t- [ ] full tx or no tx\n- [x] Compact complete\n- [ ] Components\n\t- [ ] Materialite - Rx\n\t- [ ] Mirrolite - Mirrored, read-only\n\t- [ ] Optilite - Optimistic\n- [ ] Full block based network layer and buffer for fidelity transactions\n- [ ] Read tracking ever?\n- [x] Leak test\n- [ ] sqlite expert to auto-generate indices?\n\t- [ ] https://sqlite.org/forum/forumpost/74f18021fe\n\n# P2\n- [ ] Puzzle piece connect example\n\n\n---\n\n# Notes:\n\n- Replacing column names with cids will let us fix rowid pretty easily. Since we'll have the cid for rowid compensation.\n\n- Tbl schema version? Stored in... crsql_meta? And can send over network? And compare? And reject merge if delta?\n\n\n- Ensure table infos up to date... optimized.\n\t- crsql_alter should nuke the prepared statements. but another connection could have altered. So we must ensure up to date and nuke prepareds... call once per tx? pragma schema version is the thing...\n\n```sql\nUPDATE \"foo__crsql_clock\" SET \"b\" = ? WHERE \"a\" IS ? AND __crsql_col_name != '-1'\n```\n\n## Mtg:\n\nSee if there's a way to work together. Less cagey. Improve the space overall.\n\nTypedSQL thesis is raw sql which may be wrong.\n\n- Waterfall, blinking\n- Controlled inputs\n- Transactions per second, relaxed durability or async persist\n- IVMs on main thread for sync update? DB on worker?\n\t- Rollback would roll back ivm?\n\n- Rx, papers, simple version, decomposed queries, figma live graph (https://www.figma.com/blog/livegraph-real-time-data-fetching-at-figma/)\n- GraphQL as inspo. Need trees. Component tied to query is perfection.\n- TypedSQL \u0026 TreeSQL\n\t- Needs bind param typings\n- TreeSQL\n\t- Waterfall problem\n- Permissions\n\t- What hasn't been sent problem\n- Speed run\n- Initial rx cache before full IVM\n- Controlled inputs, vanilla fetch\n\n# Notes\n\n```\n\"SELECT t1.\"a\", t1.\"b\" FROM \"foo\" AS t1\n        WHERE NOT EXISTS\n          (SELECT 1 FROM \"foo__crsql_pks\" AS t2 WHERE t1.\"a\" IS t2.\"a\" AND t1.\"b\" IS t2.\"b\")\"\n```\n\n- rowid: Mult each number by 100 and incr for cols.\n- convergent output from prior version, compare to current\n\t- expected final state compare script?\n\n## Rx Incremental for Electric\n\n- Prepare\n- De-duped queries\n- De-duped prepared (link to forum)\n- IDB Transactions (read tx and keeping open)\n- Exclusive and Relaxed IDB (streaming from IDB roy post)\n- Safari Issues (O3)\n- React event loop problems\n- --\n- WASM bridge cost\n- Async interface cost (event loop tripping)\n- Worker tripping cost\n- Batch re-run in WASM no round trip, scheduled too?\n\n\n\n---\n\n- Rowid fixup...\n- column name compaction\n\t- CIDs. They should never change unless the schema is altered...\n\t\t- Should we check them after re-pulling table info? To catch drift?\n- reactivity plan(s)\n\nA new write API that allows for synchronous updates of reactive queries.\nLazily persist in BG.\nWe need to allow partial writes then.. What if the write uses SQL to select stuff to use to write with? Then we can't get synchronous update.\n\n1. How much space savings for Strut DBs? Or vite starter?\n2. cids mean on alter we need to record the last cid set and compare against the new cid set to find removed columns and such. Convert old CIDs to names, convert new CIDs to names, find missing. A func to lookup col_name from old cid mapping and a func to look up col_name from new cid mapping. Used in backfill queries and compact queries.\n\n---\n\nYeah, there's two sides of a spectrum here:\n\nA - the DB is the single source of truth of all app state and literally every keystroke, drag, scroll, etc. goes through it.\nB - treat the DB more like some remote data store and debounce, throttle, collapse, etc. events going to it\n\nB quickly requires some new layer on top of the database (let's call it the in-memory model) since, if you're not going through the DB for everything, components need some other way to observe changes. E.g., responding to drags, hovers, keystrokes. Sometimes local component state is enough for this but often since the state may be shared across the component tree. TlDraw would be a good example of that latter case.\n\nAs you mention, dealing with bi-directional merging is also a big problem to work out since you have dirty state that diverges from DB state which can change underneath you.\n\nAnother problem is dealing with state updates to the in-memory model that fail to write to the DB. You could move as much logic as possible that could cause a transaction to fail into the in-memory model. E.g., permissions, data validation, check constraints. That's a lot of mechanics to re-create though.\n\nLast issue with B is when your in-memory model doesn't quite have the same shape as the various ways the UI needs the data.\n\n(B) or an in-memory model is certainly the traditional way of doing all of this and there's a ton of choices in that direction -- TinyBase, Signia, Mobx, ReactQuery, CoreData, etc.\n\nIn my ideal world, however, I'd go with A. I'd have some magical database that I could make any number of complex queries against and all those query results would be instantly updated on any write and all of this would take place at 60fps.\n\nThis solves:\n- UIs that need data in different formats\n- The single source of truth problem\n- Merging states between peers, tabs, devices, etc.\n- Transactionality of state updates\n- Keeping everything up to date with the latest data\n\nJohannes has actually been able to do this in his music app Overtone. Literally every keystroke, hover event, scroll, etc. is a db write and all UI components get their state directly from DB queries. We built some query caching and query de-duplication but it was pretty simple. The big problem is that you have to have a full database copy in-memory. The other thing was to make sure we didn't query data that wasn't visible. Display realestate is only so big so there's only so many queries to keep active at once.\n\nThere's been quite a number of breakthroughs in incremental view maintenance so I think A is possible now. It's a question of how much work and if there's incremental value along the way.\n\n\n\nThen there's A. I like A given it keeps dataflow unidirectional, keeps a single copy of the data, and all the mechanics of transactions and constraints stay managed by the DB.","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/Sep-4-8-2023":{"title":"Sep 4-8 2023","content":"[[Aug 28-1 2023|prev]]\n# P1\n- [ ] Metadata\n\t- [ ] Compact out primary keys\n\t- [ ] Compact out col names\n- [ ] Bugs\n\t- [x] Fix fractindex view `update x set after_id = y where id = z`\n\t- [x] Fix error message when using vtab syntax\n\t- [ ] next_db_version optimization\n\t- [x] automigrate error messaging\n\t- [ ] notify if someone overrides commit hook?\n- [ ] Docs\n\t- [x] Readme\n\t- [ ] Fractindex docs\n\t- [ ] vite starter migration flow. dbs updating on presence announce? teardown other connections in order to update schema?\n- [x] LiteFS Debug Complete\n\t- [x] https://community.fly.io/t/litefs-filesystem-notifications/15244\n- [ ] Vids\n\t- [x] Sync Starter Video\n- [ ] CI\n\t- [ ] pnpm create flow\n\t- [ ] changeset publish - https://github.com/changesets/action\n- [x] sigint / sigterm processing ws server\n- [x] rm PWA crap\n- [x] network layer reply\n- [ ] devx\n\t- [ ] typed-sql on vite-starter\n\t- [ ] migrate... migrate the server side too if db mismatch\n\t\t- [ ] does websocket server do this for us?\n\t- [ ] notify if extension version != db version\n\t- [ ] commit hash\n\t- [ ] VTab to update to make crr rather than select to make crr!\n- [ ] Future\n\t- [ ] Native networking streams\n\t- [ ] DB Forwarding litefs thing?\n- [ ] RX\n\t- [ ] Replace current RX hooks with new RX thinger\n\t- [ ] Extension doc -- https://twitter.com/agarcia_me/status/1699484306467611042\n- [ ] Admin\n\t- [x] invoice\n\t- [ ] insurance cancel\n\n# Write\n- [ ] Rx design\n- [ ] Network design\n- [ ] Vision\n- [ ] Service design\n- [ ] About page?\n\n# P2\n- [ ] RX Cache\n- [ ] Sync as sync rather than collab\n\t- [ ] VTabs for this\n\t- [ ] Incrementalism option for this by transforming crdt backed queries\n# P3\n- [ ] Extra CRDTs\n\t- [ ] Causal Log\n\t- [ ] Counter CRDT\n\t- [ ] Fugue\n\n\n# Notes\nApps have static queries and dynamic data. Oof.\nWe can index based on queries rather than data.\n\nTreeQL or whatever tells us the indices to build.\nOr `useQuery` as a static thing.\n\nLookaside change:\n1. Triggers become function invocations (unless we can figure out an insert pattern)\n\t1. Could be, as a first start, \n\t   ```sql\n\t   insert or ignore into __pks values (...);\n\t   -- need to set a variable... we could use a fn for that.\n\t   -- need to know if we conflicted...\n\t   -- actually, we can find that out with `key` since the row will already\n\t   -- exist in clock table if it'll conflict.\n\t   -- last_insert_rowid may be incorrect since we could have ignored.\n\t   -- maybe select first rather than insert or ignore so faster for\n\t   -- the already existing case\n\t   select key from __pks where pk_where_list;\n\t   \n\t   -- do normal trigger logic\n\t   ```\n\t   ","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/TreeQL":{"title":"TreeQL","content":"\n#project \n\nRelated: [[type safe sql]]\n\nHypothesis: Apps want their data in a hierarchical format.\n\n\u003e Nit: this could also be an artifact of react render cycles and asynchrony\n\nRationale: UIs are expressed in tree structures.\n\n# References\nhttps://github.com/tantaman/composed-sql","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/Untitled":{"title":"Untitled","content":"rustup toolchain install nightly-x86_64-pc-windows-gnu","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/Untitled-1":{"title":"Untitled 1","content":"We'll start by creating a new set of primitives for networking atop the existing `crsql_changes` table. This will be a higher level set of primitives that make authoring network layers simpler.\n\nThe new primitives are:\n1. InboundStream\n2. OutboundStream\n\n**An inbound stream** represents a stream of changes coming from a remote database into the local database.\n\n**An outbound stream** is a stream of changes coming from the local database to a remote.\n\nThese streams will be implemented as virtual tables. Rather than being [eponymous virtual tables](https://www.sqlite.org/vtab.html#eponymous_virtual_tables) (crsql_changes is eponymous), these will be virtual tables that can be instantiated.\n\nE.g.,\n\n```sql\nCREATE VIRTUAL TABLE out_to_peer_a USING crsql_outbound_stream(remote = peer_a_id);\nCREATE VIRTUAL TABLE in_from_peer_a USING crsql_inbound_stream(remote = peer_a_id);\n```\n\nThe Inbound/Outbound streams will assume an in-order delivery setup (primitives for out of order deliver can follow later). The Inbound/Outbound streams will manage bookkeeping for the stream to ensure changes are sent and receive in-order.\n\nTasks:\n1. [ ] Create a stub virtual table for crsql_outbound_stream. Just the wiring, don't worry about implementation yet\n    1. Here is an example of creating a vtab in Rust: https://github.com/vlcn-io/cr-sqlite/blob/main/core/rs/core/src/create_cl_set_vtab.rs\n2. [ ] Repeat for crsql_inbound_stream\n3. [ ] Implement the ability to `SELECT` from the outbound stream. See [select from outbound stream](#select_from_outbound_stream)\n4. [ ] Implement the ability to `INSERT` into the inbound stream\n\n\n\n# Select From Outbound Stream\n\n```sql\nSELECT change, cursor FROM outbound_stream WHERE cursor \u003e :last_cursor\n```\n\nThe outbound stream is only queryable by cursor. The `change` column contains all values that describe the change, packed together in a binary format. We can re-use this logic for packing columns: https://github.com/vlcn-io/cr-sqlite/blob/main/core/rs/core/src/pack_columns.rs\n\nThe underlying implementation delegates to `crsql_changes`\n\n# Step the Outbound Stream\n\n\n\n# Insert into Inbound Stream","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/Untitled-2":{"title":"Untitled 2","content":"```\nimport \"./styles.css\";\nimport {useCallback, useState} from \"react\"\n\nexport default function App() {\n  const [value, setValue] = useState('');\n  const [taskValue, setTaskValue] = useState('');\n  subscriber = useCallback((v) =\u003e {\n    setValue(v);\n  }, []);\n  taskSubscriber = useCallback((v) =\u003e {\n    setTaskValue(v)\n  }, []);\n\n  return (\n    \u003cdiv className=\"App\"\u003e\n      Micro task input: \u003cinput type=\"text\" value={value} onChange={(e) =\u003e {persist(e.target.value)}} /\u003e\n      \u003cbr/\u003e\u003cbr/\u003e\n      Task input: \u003cinput type=\"text\" value={taskValue} onChange={(e) =\u003e {persistWithTask(e.target.value)}} /\u003e\n    \u003c/div\u003e\n  );\n}\n\nlet subscriber;\nlet taskSubscriber;\n\nfunction persist(v) {\n  queueMicrotask(() =\u003e {\n    subscriber(v)\n  })\n}\n\nfunction persistWithTask(v) {\n  setTimeout(() =\u003e {\n    taskSubscriber(v)\n  }, 0);\n}\n```\n\nhttps://github.com/facebook/react/issues/24365\nhttps://github.com/facebook/react/issues/24625","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/blogs":{"title":"blogs","content":"```dataview\ntable priority from #blog sort priority asc\n```\n","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/bloom-filters-for-row-level-security":{"title":"bloom filters for row level security","content":"\nBloom filters don't so much implement the security mechanism as they enable tracking dirty bits so we can still incrementally send updates after privacy invalidations rather than requiring full re-pulls.\n\n![[row level security#^d9509b]]\n\nSo how would a bloom filter better integrate row level security and [[incrementalism]]?\n\nThe idea would be that each time a permission rule denies a read we'd record the primary keys of the denied row in a bloom filter.\n\n\u003e hm.. is this a bloom filter _per client_ ? :/ Is that tenable?\n\nWhen permissions change, exposing a new set of available data, we'd pass the primary keys of this data through the bloom filter. All keys possibly in the set of denied reads would be enqueued to be sent to the client.\n\n\u003e ok, but the question is: how do we efficiently compute the set of newly available data?\n\nI think we need to pair with query based sync and understanding the \"active set\" of queries the client wants to sync (i.e., [[client defined query based sync]]). Looking back at the [[user promoted to god example]], we wouldn't ever want to sync that user _all data on the site_.","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/blurring-the-client-server-boundary-might-be-a-mistake":{"title":"blurring the client-server boundary might be a mistake","content":"Thinking along the lines of GraphQL \u0026 Relay which may blend it but still make it obvious by requiring fragments to be declared statically.\n\nIf we don't let the dev define static queries of data needs we can't load efficiently.\n\nIt isn't the blurring that is an issue. Its the not having static access to data needs that is.","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/client-defined-query-based-sync":{"title":"client defined query based sync","content":"\nHow much data does the client device need?\n\n- Whole database replication may be untenable for situations where the user's devices have different storage profiles\n- Whole database replication is a non-starter for traditional application architectures and all data for all users in the same db\n- Whole db replication doesn't make sense when sharing is not at the DB level\n\nAnother issue is what data to prioritize for sync / fetch and when. Blindly syncing the DB without input from the app dev may lead to a bad experience.\n\nThink about [[user promoted to god example]]. We don't want to block all other syncs as the entire set of what they now have access to streams in.\n\nSo what is client defined query based sync? And why is it possible harder or different in a local-first architecture?\n\n[[blurring the client-server boundary might be a mistake]]\n\n# What it is\n\nTraditional client-server apps will have the client request some set of data from the server for each view. E.g., either via a Rest endpoint for the view or GraphQL.\n\nThis ensures each view only fetches what it needs.\n\nWe can take the same approach with the local-first model but maybe additionally supply data to continue loading into the background.\n\nThe issue with these client defined queries and our current model, however, is as follows:\n1. [[incrementalism]]. We'd need to know how to incrementally update these queries as the underlying data changes.\n2. [[row level security]]. We'd need to know how to update these queries as the set of visible data to the user changes\n3. Merge metadata. The results of these queries, when querying CRRs, need to include merge metadata.\n4. Mixing models. Ideally the user can mix strongly and eventually consistent data.\n\nIf we re-cast everything in this model... we no longer just wire dbs directly together over the sync protocol.\n\n# Getting Started\n\nWhat is the simplest way to start supporting this model?\n\n- We'd need the inverted DB to support subscriptions\n- Each `useQuery` can set up a subscription with the backend\n\t- Of course multiplexed over a single connection\n- LiteFS issue... we just know of DB change not what changed. You could get \"what\" via a db_version query and map that what to subscribers.\n- how do we resume a userQuery? Need to know the db_version it last left off at + user's bloom filter of restricted reads.\n\t- Run the query only grabbing rows past the given db_version?\n\t\t- but need to get \"now visible due to privacy\" rows which could be in the past\n\t- Run the full query, returning all rows?\n\t- Run the full query\n\t\t- Return rows that are \u003e db_version\n\t\t- Return rows that are \u003c db_version but possibly in the bloom filter\n\nDevs should get control over how this process works.\n\nSynchronous queries that resolve immediately based on hydrated subscriptions followed by data from disk followed by data from network...\n\nOr should we just make everything explicit to the dev instead of trying to hide it?\n\n[[mem -\u003e disk -\u003e network]]\n\nQueries to DB then with eventual data over the network is a new model. E.g., `SELECT * FROM foo JOIN bar`. Maybe no data exists locally for `bar` yet.\n\n\n\nThis is the root of our problem and a problem traditional apps easily solve by forcing the client to specify queries to be made of the server.\n\n\n---\nscratch:\nA confluence of problems make this architecture a requirement.\n\n1. Incrementally fetching changed data\n2. Row level security and incrementally returning what the user can now see\n\t1. the [[user promoted to god example]]\n3. - ","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/converting-relations-to-hierarchies":{"title":"converting relations to hierarchies","content":"\n#blog\n\nApplications generally need their data in a hierarchical format. E.g., a UI is a tree where data is passed down the tree to hydrate components.\n\nRelation are of course great -- they let us view the data in any tree like structure we want whereas if we start with a tree we're stuck in that singular format.\n\nWe just need a better way to turn relations into trees. SQL is great in that it maps from relations to relations so we can continue using the same computation primitives against query results as we use against base tables but it was not made to pull a hierarchy.\n\n# Options\n- SQL messaging via JSON group bys:  https://twitter.com/schickling/status/1599076832107630594\n- Drizzle [relation queries](https://orm.drizzle.team/docs/rqb)\n- `useQuery` at each component to pull the inherently flat data at a given level of the tree\n\t- This leads to waterfalling and flickering problems in the case where the storage API is async. E.g., your pokemon game example","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/data-model-maturity-curve":{"title":"data model maturity curve","content":"\n#blog\n\nData models in UI applications go through a maturity curve.\n\n1. They start as fully in-memory with some lazy persist of the entire blob over the network\n2. Fully in-memory with the blob split up into chunks that can go over the network\n\t1. These chunks are specific to the application data format\n3. Fully in-memory, chunked networking\n4. Fully in-memory with some lazy local persist to disk + lazy persist over network\n\t1. This is to facilitate faster initial startup\n5. ","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/dataview-into-hugo":{"title":"dataview into hugo","content":"\nhttps://github.com/blacksmithgu/obsidian-dataview/issues/42\n\nLooks like people use [templater](https://github.com/SilentVoid13/Templater) to generate static markdown and refresh on occasion:\n\n```\n\u003c%*  \nconst dv = app.plugins.plugins[\"dataview\"].api;\nconst te = await dv.queryMarkdown(`TASK FROM #journal WHERE contains(text, \"${tp.file.title}\")`);\ntR += te.value;\n%\u003e\n```\n","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/db-from-scratch":{"title":"db from scratch","content":"\n#project\n\nStart with kv store:\n- https://github.com/cberner/redb\n\nSQL interpreter on top:\n- pull vtab mechanism from SQLite?","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/debug-tools":{"title":"debug tools","content":"\n#project \n\n- Number of queries per event loop tick\n- Query log\n- Component update log (which component, which query caused it, which event caused the query)\n- FPS meter\n- Number of queries per UI event\n\nWay there --\nAdopt Riffle and the reactive graph?","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/differential-data-flow":{"title":"differential data flow","content":"\n# Resources\n- https://github.com/TimelyDataflow/differential-dataflow\n- https://github.com/wotbrew/relic\n","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/evolution-of-data-compute-use-colocation":{"title":"evolution of data, compute, use colocation","content":"90s: User, data, compute colocated\nCloud: data \u0026 compute colocated. User separated.\nMap-Reduce: Data distributed, compute moved to data, user separated.\n\"Edge\": Data moved to compute. Data could be moved to user.\n\n[[Beyond CRDTs, The Next Frontier of Local-First]]\n\nThe next local first question:\nHow do I get my data where I need it, when I need it?\n\nCloud answered this:\nRe-query the central service each time. Queries driven by current view of the user. Anything already on device is just a cache.\n\nCurrent work to improve the answer:\nPre-fetch views the user may go to. Still full queries, still treat responses as cache.\nMove more data to servers near user for lower latency\n\nLocal-First:\nCan we take the cloud approach?\n- Your dataset is more expansive than your current view.\n- In LoFi, On-device is not just a cache. Could re-query all then merge.\n- Can we do incrementally?","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/front-page-story":{"title":"front page story","content":"#project \n\n1. Relational: Defining tables\n2. Reactive: Querying tables / react integration\n3. Replicated: CRR Upgrade\n4. Real-time: something something lazy\n5. Transactional?","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/fully-synchronous-reactivity":{"title":"fully synchronous reactivity","content":"\n#blog #project\n\nA problem that is introduced once an app moves from having all data in-memory to having to page some data to disk is that all access to that data suddenly becomes async. The app's entire data model, even if there is an ORM'ed version of it, would likely be completely infected with async as we can't always know when we'll need to wait and go to disk.\n\nWe can start to solve this problem, however, with subscriptions.","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/incrementalism":{"title":"incrementalism","content":"\nEvery write to the database receives a corresponding database version. The database version is a continuously increasing 64 bit int. It increase on every transaction. All writes in the same transaction receive the same db version.\n\n- Inserting a row associates all columns in that row with the next db version\n- Updating a column associates that column with the next db version\n- Deleting a row records a sentinel for that row with the next db version\n\nThis mapping of mutations to database versions allows for efficient incremental updates. A node just asks another node for all changes since the db version it last received.\n\nWhen met with [[row level security]], however, this solution to incrementalism is not enough.","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/ink-and-switch-lofi-notes":{"title":"ink and switch lofi notes","content":"\nhttps://www.inkandswitch.com/local-first/\n\n\u003e In this article we propose “local-first software”: a set of principles for software that enables both collaboration _and_ ownership for users\n\n\u003e “local-first software”: a set of principles for software that enables both collaboration _and_ ownership for users. Local-first ideals include the ability to work offline and collaborate across multiple devices, while also improving the security, privacy, long-term preservation, and user control of data.\n\n\n\n- Software, in the 90s and early 2000's, was local.\n- Local-first is _strictly harder_ / software on hard-mode\n\t- Harder then 90s local\n\t- Harder than cloud\n- 90s concept of local just can't work today\n\t- Self Sync\n\t- Collab\n\t- Different device constraints but same app on all\n\n\nWe've been surrounded by \"local\" software\n- Mail client\n- File explorer\n- Orig iTunes\n- Orig Office\n\nAnd the apple ecosystem:\n- iPhoto\n- iTunes of today?\n\nHow did they handle self sync and collab? Through the unit of files.\n\nDoes the file unit make sense in today's world?\n- Issue tracking system. No. Each issue a file? Each comment? Private comments and threads?\n- iPhoto -- I don't have to deal with file management. I.e., what to place on cloud vs on device. iPhoto does it.\n\nThe migration to the cloud made sense. It was an exodus from the \"final-final\" copies of docs. The use driven management of \"where to store a thing.\"\n\nBut with the migration to the cloud we lost:\n- Offline capability\n- Data ownership / preservation\n\nLocal-first is all the benefits of the cloud:\n- Available on any device (if network)\n- Not thinking about storage (remember diskmon?)\n- Sharing at fine grained levels, not just at the file\n\nwe started local. We got devices, we got networked. We saw the problems (final-final, disk full, where to put a thing, can only share at file level... JIRA over excel? hmm). Exodus to cloud made sense. Now we see the problems of the cloud:\n- Where'd my data go?\n- Where'd the product go?\n- What's the storage format? How can I exit? Export to other apps?\n- My data isn't my data. It is behind a wall.\n- Innovation is stifled\n- No internet no go? Wut?\n- Latency :|\n\n\nThought experiment: JIRA over Excel file?\nThat excel file needs:\n- Row level security\n- Sync\n- Merging\n- Finding peers (or facilitated by server)\n- Access control\n- Auth\n\nMuch is said about \"end user programming\" and how \"excel enables this\" but could we ever build a cloud like app out of excel? One with sharing and collaboration and permissions and such?\n\nBut how do we go local and keep the advantages of the cloud?\nFirst, what does the cloud give us:\n- Fine grained (row level, column level) access control\n- No thinking about storage\n- Single repository of all information, no final-final\n- Collaboration on the same item(s)\n- Auth\n\nLocal-first software runs even if the service shuts down. What does this mean?\n- Copies of data on own device(s)\n- Software runs w/o network on available local set\n- Sync protocol so we can get new sync providers?\n\t- but what if there is a required cloud component of server authoritative data?\n\t- can this be encoded in the protocol? Service logic is made available for new services to spin up when original spins down?\n\nWe can rabbit-hole on storage first.\n\n","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/inverted-database":{"title":"inverted database","content":"\n#project\n\nThe reactivity problem is a matter of answering the question:\n\u003e For a given write, what queries are invalidated by that write?\n\nIn as fine a grained way as possible. I.e., only invalidating the exact queries that dependended on the data written rather than invalidating a large blast radius.\n\nOne step to solving this is noticing that queries specify ranges:\n\n`SELECT * FROM foo WHERE x \u003e ? AND x \u003c ?`\n\nAnd points:\n\n`SELECT * FROM foo WHERE x = ?`\n\nand that we can \"index the queries\" themselves by indexing the ranges and/or points they are requesting.\n\nAn R-Tree is ideal for this. We could use SQLite in-memory and it's R-Tree to index our queries if the [[wasm tax]] isn't too high.\n\n# Joins\n\n# References\n- https://github.com/ccorcos/tuple-database#Reactivity\n- https://github.com/wotbrew/relic#reactive-programming","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/july-10-14-2023":{"title":"july 10-14, 2023","content":"\nhttps://github.com/sqlite/sqlite/blob/master/src/prepare.c#L684\n\ntest that cache is not growing w/o bound? you have a cache test tho righ?\n\n- CRDT Base\n\t- [ ] p1: re-insertion\n\t- [ ] p1: col level trigger rather than table level so we can deal with pk adjustment\n\t- [x] p1: warn on auto-increment use\n\t- [ ] p1: automigrator stability\n\t- [x] p2: unpack columns table valued function\n\t- [ ] p2: vtab syntax\n\t- [ ] p2: turso crsqlite direct from browser\n\t- [ ] vite-express rather than forking a process?\n\t- [x] test p2p still works with new networking code..\n- Fly.io\n\t- [ ] provide count of changed db versions post merge\n- DevX / Starting\n\t- [ ] Hooks for read-only access or other permission rules\n\t- [ ] Document plan for [[row level security]]\n- TypedSQL\n\t- [x] split that PR into `typed-sql` and `typed-db`\n- Other\n\t- [ ] Webkit bug report for WASM JIT\n\t- [x] [[Beyond CRDTs, The Next Frontier of Local-First]]\n\t\t- [x] https://2023.splashcon.org/home/plf-2023\n\t- [ ] Garden index\n- Riffle\n\t- [ ] IVM / inverted DB approach\n\t\t- [ ] Hash - needs to be varargs.\n\t\t- https://github.com/nalgeon/sqlean/blob/ab0e7e2173328fadad492673fbdd4851a6d3f9e6/src/crypto/extension.c#L198\n\t\t- https://github.com/nalgeon/sqlean/blob/main/src/crypto/sha1.c\n- Vision\n\t- [ ] Frontpage guide\n\t\t- [ ] automigrator\n\t- [x] [[Beyond CRDTs, The Next Frontier of Local-First]]\n\nwhat actually happened:\n- typed sql\n- execution distraction\n- start rust conversion\n\n# Scratch\ncol-specific-triggers:\n- update on pk ... always means delete of the old thing?\n- insert on pk ... don't need col specific triggers for pk insert. Only on insert of row to create or increment the CL sentinel.\n- delete on pk ... don't need. Only need delete on row to update sentinel.\n\nImplement delete-wins / existing behavior with CL sentinel instead of pko and delete sentinels?\nThen move it to CL proper?\n\n- add pko on insert\n\nSo we only need col specific triggers for update?\nInserts should overwrite clock values anyway for the new model given we compare on CL.\n\nYeah, I'm sure we're coming from pretty different backgrounds.\n\nBacking up --\n\nThere's a bunch of problems when it comes to managing state on a rich client like a mobile app or single page app in the browser. Once you throw sync and multiplayer into the mix it gets really hard.\n\nMy main thesis is that client side applications would benefit greatly from leveraging a database. The use of SQLite is pretty common on iOS and Android but the database still doesn't handle reactivity, syncing offline edits, loading data from the server, or multiplayer in those environments.\n\n- Reactivity -- The lack of reactivity means data flow often becomes a tangled web of observers\n- Syncing Offline Edits -- Not being able to sync changes made while offline means the developer finds themselves implementing it themselves through some operation queue and manually writing conflict resolution code.\n- Loading data from the server -- this is the most radical but if the client DB could load data from the server (just treating it as another storage layer after the disk) data fetching should get simpler on the client.\n- Multiplayer -- this is similar to the making changes while offline. Developers are left to their own devices to figure out how to implement multiplayer experiences.\n\nSo I have pretty strong opinions on what the solution looks like of course always open to real world feedback of users.\n\n\n\nThe general maturity curve for state in an app is something like:\n\n1. Keep everything in memory. Save and load the entire blob on app open and exit and maybe some interval\n2. The blob starts getting big so devs partition it into different documents\n3. \n\nMy main thesis is that frontend development would benefit greatly from having a database on the frontend.","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/keeping-your-data-model-synchronous":{"title":"keeping your data model synchronous","content":"\n#blog\n\nA problem that is introduced once an app moves from having all data in-memory to having to page some data to disk is that all access to that data suddenly becomes async. The app's entire data model, even if there is an ORM'ed version of it, would likely be completely infected with async as we can't always know when we'll need to wait and go to disk.\n\n[[fully synchronous reactivity]]\n[[data model maturity curve]]","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/licensing-example":{"title":"licensing example","content":"https://rocicorp.dev/terms.html\n\n```\nLicense pings\nPer Replicache Pricing, we charge post-funding/revenue commercial customers based on Monthly Active Browser Profiles, meaning unique browser instances that instantiate Replicache in a calendar month. The way we accomplish this is to send a ping to our servers containing your license key and a unique browser profile identifier when Replicache is instantiated, and every 24 hours that it is running. We also check at instantiation time that your license key is valid, and complain loudly to the console if it is not. We may in the future add a feature to disable Replicache in the event that the license key is not valid.\n\nThe licensing pings explain why you want to pass TEST_LICENSE_KEY to Replicache in automated tests: so that you're not potentially charged for large numbers of Replicache instances used when running tests. (Not to mention network calls are typically undesirable in unit tests).\n\nDisabling Replicache's pings other than via the TEST_LICENSE_KEY is against our Terms of Service. If the pings are a problem for your environment, please get in touch with us at hello@replicache.dev.\n```","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/lofi-presentations":{"title":"lofi presentations","content":"\n- [[Beyond CRDTs, The Next Frontier of Local-First]]\n- [[ink and switch lofi notes]]","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/logging":{"title":"logging","content":"#project\n\nTS components need more logging.\n\n[Winston](https://github.com/winstonjs/winston) for both [browser](https://stackoverflow.com/questions/51110058/can-winston-logger-be-used-on-the-front-end-for-logging) and server.","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/mem-disk-network":{"title":"mem -\u003e disk -\u003e network","content":"\nWe're issuing SQL.\n\nThere's three levels of data availability:\n- Memory\n- Disk\n- Network\n\nOne way to approach this is to try to make it all transparent to the dev:\n```\n- issue a synchronous query\n- get empty result back\n- subscription then keeps them up to date with whatever flows in from disk or network\n```\n\nOf course we have transition problems here if we want to wait for data or be aware of loading states.\n\n\u003e Do we ever know if we have the complete / final set of data for a query? We could know of some finality with respect to a delta with the server... \"final as of last sync\" or some such. Finality with respect to when _that specific query_ was synced... if we do client defined query sync.\n\n```ts\nconst x = useQuery(...);\n```\n\nYour `useQuery` hook already exposes these states! Excluding network! Via the return value:\n\n```ts\nreturn {\n  data: \u003c-- memory resident data,\n  loading: \u003c-- loading status\n}\n```\n\nWe can extend the loading status to indicate network \u0026 disk and with what finality the network is. If the subscription is hydrated we often never need to hit disk.","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/north-star":{"title":"north star","content":"The embedded database for the application needs of the next three decades.\n\n- Reactive\n- Synced\n- Offlinable\n- RLS\n- Tightly integrated into host languages\n- Tiered storage\n- Mixed Consistency Support\n\nA full developer experience centered around the DB.\n- Lang integration (types, query lang?)\n- Automigrate\n- Post-facto relational?\n- \n\nEventually evolve into its own DB and graduate from SQLite.\nEvolution plan is based around the reactivity system since we'll start to build a query processor at that level.\n\nBrownfield plans...\n\nMilestones:\n- [x] CRDTs\n- [ ] Text CRDTs\n- [ ] Efficiency of CRDTs\n- [ ] Smart Reactivity\n- [ ] Row Level Security\n- [ ] Query Based Sync\n- [ ] Multi-tenant DB support\n- [ ] Tiered Storage\n- [ ] Mixed Consistency","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/prioritization-legend":{"title":"prioritization legend","content":"\n- P1 - Will definitely get done\n- P2 - At least 50% of these will get done\n- P3 - If there's any extra time","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/product-vision":{"title":"product vision","content":"\n- Client\n\t- crdts for local writes\n\t\t- but we still need server overrides\n\t\t- and strong consistency\n\t\t- and \"optimistic only\" tables / rows\n\t\t- What about CRDT fields that should be \"set together\"\n\t\t- JSON story (property table)\n\t- Reactivity\n\t- Functional Relational Programming\n\t- Rich text\n\t- [[converting relations to hierarchies|hierarchical fetch]]\n- Multiple consistency models\n\t- CRDT / Causal\n\t- Strongly consistent, read-only, mirrored tables\n\t- Mirrored tables which allow client-side optimistic writes\n\t- Strongly consistent, read/write, mirrored tables\n- Storage\n\t- What if the entire DB can't fit on device? How much do we put there? [Tiered storage?](https://rocksdb.org/blog/2022/11/09/time-aware-tiered-storage.html)\n- Backend\n\t- multi-tenancy\n\t- subscriptions\n\t- row level security\n\t- sync a subset of the database\n- Network\n\t- Client-server mainly\n\t\t- Server overrides\n\t\t- Server permissions / rejections\n\t- P2P never precluded by CRDT design decisions\n\n# References\n- https://riffle.systems/\n- https://github.com/papers-we-love/papers-we-love/blob/main/design/out-of-the-tar-pit.pdf\n- ","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/projects":{"title":"projects","content":"```dataview\ntable priority from #project sort priority asc\n```\n- re-insertion\n- primary key modifications (delete then insert)\n- gh issue list\n","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/quartz-docs/CJK-+-Latex-Support-%E6%B5%8B%E8%AF%95":{"title":"CJK + Latex Support (测试)","content":"\n## Chinese, Japanese, Korean Support\n几乎在我们意识到之前，我们已经离开了地面。\n\n우리가 그것을 알기도 전에 우리는 땅을 떠났습니다.\n\n私たちがそれを知るほぼ前に、私たちは地面を離れていました。\n\n## Latex\n\nBlock math works with two dollar signs `$$...$$`\n\n$$f(x) = \\int_{-\\infty}^\\infty\n    f\\hat(\\xi),e^{2 \\pi i \\xi x}\n    \\,d\\xi$$\n\t\nInline math also works with single dollar signs `$...$`. For example, Euler's identity but inline: $e^{i\\pi} = -1$\n\nAligned equations work quite well:\n\n$$\n\\begin{aligned}\na \u0026= b + c \\\\ \u0026= e + f \\\\\n\\end{aligned}\n$$\n\nAnd matrices\n\n$$\n\\begin{bmatrix}\n1 \u0026 2 \u0026 3 \\\\\na \u0026 b \u0026 c\n\\end{bmatrix}\n$$\n\n## RTL\nMore information on configuring RTL languages like Arabic in the [config](quartz-docs/config.md) page.\n","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/quartz-docs/callouts":{"title":"Callouts","content":"\n## Callout support\n\nQuartz supports the same Admonition-callout syntax as Obsidian.\n\nThis includes\n- 12 Distinct callout types (each with several aliases)\n- Collapsable callouts\n\nSee [documentation on supported types and syntax here](https://help.obsidian.md/Editing+and+formatting/Callouts).\n\n## Showcase\n\n\u003e [!EXAMPLE] Examples\n\u003e\n\u003e Aliases: example\n\n\u003e [!note] Notes\n\u003e\n\u003e Aliases: note\n\n\u003e [!abstract] Summaries \n\u003e\n\u003e Aliases: abstract, summary, tldr\n\n\u003e [!info] Info \n\u003e\n\u003e Aliases: info, todo\n\n\u003e [!tip] Hint \n\u003e\n\u003e Aliases: tip, hint, important\n\n\u003e [!success] Success \n\u003e\n\u003e Aliases: success, check, done\n\n\u003e [!question] Question \n\u003e\n\u003e Aliases: question, help, faq\n\n\u003e [!warning] Warning \n\u003e\n\u003e Aliases: warning, caution, attention\n\n\u003e [!failure] Failure \n\u003e\n\u003e Aliases: failure, fail, missing\n\n\u003e [!danger] Error\n\u003e\n\u003e Aliases: danger, error\n\n\u003e [!bug] Bug\n\u003e\n\u003e Aliases: bug\n\n\u003e [!quote] Quote\n\u003e\n\u003e Aliases: quote, cite\n","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/quartz-docs/config":{"title":"Configuration","content":"\n## Configuration\n\nQuartz is designed to be extremely configurable. You can find the bulk of the configuration scattered throughout the repository depending on how in-depth you'd like to get.\n\nThe majority of configuration can be found under `data/config.yaml`. An annotated example configuration is shown below.\n\n```yaml {title=\"data/config.yaml\"}\n# The name to display in the footer\nname: Jacky Zhao\n\n# whether to globally show the table of contents on each page\n# this can be turned off on a per-page basis by adding this to the\n# front-matter of that note\nenableToc: true\n\n# whether to by-default open or close the table of contents on each page\nopenToc: false\n\n# whether to display on-hover link preview cards\nenableLinkPreview: true\n\n# whether to render titles for code blocks\nenableCodeBlockTitle: true\n\n# whether to render copy buttons for code blocks\nenableCodeBlockCopy: true\n\n# whether to render callouts\nenableCallouts: true\n\n# whether to try to process Latex\nenableLatex: true\n\n# whether to enable single-page-app style rendering\n# this prevents flashes of unstyled content and improves\n# smoothness of Quartz. More info in issue #109 on GitHub\nenableSPA: true\n\n# whether to render a footer\nenableFooter: true\n\n# whether backlinks of pages should show the context in which\n# they were mentioned\nenableContextualBacklinks: true\n\n# whether to show a section of recent notes on the home page\nenableRecentNotes: false\n\n# whether to display an 'edit' button next to the last edited field\n# that links to github\nenableGitHubEdit: true\nGitHubLink: https://github.com/vlcn-io/knowledgebase/tree/hugo/content\n\n# whether to render mermaid diagrams\nenableMermaid: true\n\n# whether to use Operand to power semantic search\n# IMPORTANT: replace this API key with your own if you plan on using\n# Operand search!\nsearch:\n  enableSemanticSearch: false\n  operandApiKey: \"REPLACE-WITH-YOUR-OPERAND-API-KEY\"\n  operandIndexId: \"REPLACE-WITH-YOUR-OPERAND-INDEX-ID\"\n\n# page description used for SEO\ndescription:\n  Host your second brain and digital garden for free. Quartz features extremely fast full-text search,\n  Wikilink support, backlinks, local graph, tags, and link previews.\n\n# title of the home page (also for SEO)\npage_title: \"🪴 Quartz 3.3\"\n\n# links to show in the footer\nlinks:\n  - link_name: Twitter\n    link: https://twitter.com/vlcnio\n  - link_name: Github\n    link: https://github.com/vlcn-io\n```\n\n### Code Block Titles\n\nTo add code block titles with Quartz:\n\n1. Ensure that code block titles are enabled in Quartz's configuration:\n\n   ```yaml {title=\"data/config.yaml\", linenos=false}\n   enableCodeBlockTitle: true\n   ```\n\n2. Add the `title` attribute to the desired [code block\n   fence](https://gohugo.io/content-management/syntax-highlighting/#highlighting-in-code-fences):\n\n   ````markdown {linenos=false}\n   ```yaml {title=\"data/config.yaml\"}\n   enableCodeBlockTitle: true # example from step 1\n   ```\n   ````\n\n**Note** that if `{title=\u003cmy-title\u003e}` is included, and code block titles are not\nenabled, no errors will occur, and the title attribute will be ignored.\n\n### HTML Favicons\n\nIf you would like to customize the favicons of your Quartz-based website, you\ncan add them to the `data/config.yaml` file. The **default** without any set\n`favicon` key is:\n\n```html {title=\"layouts/partials/head.html\", linenostart=15}\n\u003clink rel=\"shortcut icon\" href=\"icon.png\" type=\"image/png\" /\u003e\n```\n\nThe default can be overridden by defining a value to the `favicon` key in your\n`data/config.yaml` file. For example, here is a `List[Dictionary]` example format, which is\nequivalent to the default:\n\n```yaml {title=\"data/config.yaml\", linenos=false}\nfavicon:\n  - { rel: \"shortcut icon\", href: \"icon.png\", type: \"image/png\" }\n#  - { ... } # Repeat for each additional favicon you want to add\n```\n\nIn this format, the keys are identical to their HTML representations.\n\nIf you plan to add multiple favicons generated by a website (see list below), it\nmay be easier to define it as HTML. Here is an example which appends the\n**Apple touch icon** to Quartz's default favicon:\n\n```yaml {title=\"data/config.yaml\", linenos=false}\nfavicon: |\n  \u003clink rel=\"shortcut icon\" href=\"icon.png\" type=\"image/png\"\u003e\n  \u003clink rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"/apple-touch-icon.png\"\u003e\n```\n\nThis second favicon will now be used as a web page icon when someone adds your\nwebpage to the home screen of their Apple device. If you are interested in more\ninformation about the current and past standards of favicons, you can read\n[this article](https://www.emergeinteractive.com/insights/detail/the-essentials-of-favicons/).\n\n**Note** that all generated favicon paths, defined by the `href`\nattribute, are relative to the `static/` directory.\n\n### Graph View\n\nTo customize the Interactive Graph view, you can poke around `data/graphConfig.yaml`.\n\n```yaml {title=\"data/graphConfig.yaml\"}\n# if true, a Global Graph will be shown on home page with full width, no backlink.\n# A different set of Local Graphs will be shown on sub pages.\n# if false, Local Graph will be default on every page as usual\nenableGlobalGraph: false\n\n### Local Graph ###\nlocalGraph:\n  # whether automatically generate a legend\n  enableLegend: false\n\n  # whether to allow dragging nodes in the graph\n  enableDrag: true\n\n  # whether to allow zooming and panning the graph\n  enableZoom: true\n\n  # how many neighbours of the current node to show (-1 is all nodes)\n  depth: 1\n\n  # initial zoom factor of the graph\n  scale: 1.2\n\n  # how strongly nodes should repel each other\n  repelForce: 2\n\n  # how strongly should nodes be attracted to the center of gravity\n  centerForce: 1\n\n  # what the default link length should be\n  linkDistance: 1\n\n  # how big the node labels should be\n  fontSize: 0.6\n\n  # scale at which to start fading the labes on nodes\n  opacityScale: 3\n\n### Global Graph ###\nglobalGraph:\n  # same settings as above\n\n### For all graphs ###\n# colour specific nodes path off of their path\npaths:\n  - /moc: \"#4388cc\"\n```\n\n## Styling\n\nWant to go even more in-depth? You can add custom CSS styling and change existing colours through editing `assets/styles/custom.scss`. If you'd like to target specific parts of the site, you can add ids and classes to the HTML partials in `/layouts/partials`.\n\n### Partials\n\nPartials are what dictate what gets rendered to the page. Want to change how pages are styled and structured? You can edit the appropriate layout in `/layouts`.\n\nFor example, the structure of the home page can be edited through `/layouts/index.html`. To customize the footer, you can edit `/layouts/partials/footer.html`\n\nMore info about partials on [Hugo's website.](https://gohugo.io/templates/partials/)\n\nStill having problems? Checkout our [FAQ and Troubleshooting guide](quartz-docs/troubleshooting.md).\n\n## Language Support\n\n[CJK + Latex Support (测试)](\u003cquartz-docs/CJK%20+%20Latex%20Support%20(测试).md\u003e) comes out of the box with Quartz.\n\nWant to support languages that read from right-to-left (like Arabic)? Hugo (and by proxy, Quartz) supports this natively.\n\nFollow the steps [Hugo provides here](https://gohugo.io/content-management/multilingual/#configure-languages) and modify your `config.toml`\n\nFor example:\n\n```toml\ndefaultContentLanguage = 'ar'\n[languages]\n  [languages.ar]\n    languagedirection = 'rtl'\n    title = 'مدونتي'\n    weight = 1\n```\n","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":["setup"]},"/quartz-docs/custom-Domain":{"title":"Custom Domain","content":"\n### Registrar\nThis step is only applicable if you are using a **custom domain**! If you are using a `\u003cYOUR-USERNAME\u003e.github.io` domain, you can skip this step.\n\nFor this last bit to take effect, you also need to create a CNAME record with the DNS provider you register your domain with (i.e. NameCheap, Google Domains).\n\nGitHub has some [documentation on this](https://docs.github.com/en/pages/configuring-a-custom-domain-for-your-github-pages-site/managing-a-custom-domain-for-your-github-pages-site), but the tldr; is to\n\n1. Go to your forked repository (`github.com/\u003cYOUR-GITHUB-USERNAME\u003e/quartz`) settings page and go to the Pages tab. Under \"Custom domain\", type your custom domain, then click **Save**.\n2. Go to your DNS Provider and create a CNAME record that points from your domain to `\u003cYOUR-GITHUB-USERNAME.github.io.` (yes, with the trailing period).\n\n\t![Example Configuration for Quartz](quartz-docs/images/google-domains.png)*Example Configuration for Quartz*\n3. Wait 30 minutes to an hour for the network changes to kick in.\n4. Done!","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/quartz-docs/docker":{"title":"Hosting with Docker","content":"\nIf you want to host Quartz on a machine without using a webpage hosting service, it may be easier to [install Docker Compose](https://docs.docker.com/compose/install/) and follow the instructions below than to [install Quartz's dependencies manually](quartz-docs/preview%20changes.md).\n## Hosting Quartz Locally\nYou can serve Quartz locally at `http://localhost:1313` with the following script, replacing `/path/to/quartz` with the \nactual path to your Quartz folder.\n\ndocker-compose.yml\n```\nservices:\n  quartz-hugo:\n    image: ghcr.io/jackyzha0/quartz:hugo\n    container_name: quartz-hugo\n    volumes:\n      - /path/to/quartz:/quartz\n    ports:\n      - 1313:1313\n\n    # optional\n    environment:\n      - HUGO_BIND=0.0.0.0\n      - HUGO_BASEURL=http://localhost\n      - HUGO_PORT=1313\n      - HUGO_APPENDPORT=true\n      - HUGO_LIVERELOADPORT=-1\n```\n\nThen run with: `docker-compose up -d` in the same directory as your `docker-compose.yml` file.\n\nWhile the container is running, you can update the `quartz` fork with: `docker exec -it quartz-hugo make update`.\n\n## Exposing Your Container to the Internet\n\n### To Your Public IP Address with Port Forwarding (insecure)\n\nAssuming you are already familiar with [port forwarding](https://en.wikipedia.org/wiki/Port_forwarding) and [setting it up with your router model](https://portforward.com):\n\n1. You should set the environment variable `HUGO_BASEURL=http://your-public-ip` and then start your container.\n2. Set up port forwarding on your router from port `p` to `your-local-ip:1313`.\n3. You should now be able to access Quartz from outside your local network at `http://your-public-ip:p`.\n\nHowever, your HTTP connection will be unencrypted and **this method is not secure**.\n\n### To a Domain using Cloudflare Proxy\n\n1. Port forward 443 (HTTPS) from your machine.\n2. Buy a custom domain (say, `your-domain.com`) from [Cloudflare](https://www.cloudflare.com/products/registrar/). Point a DNS A record from `your-domain.com` to your public IP address and enable the proxy.\n3. Set the environment variables `HUGO_BASEURL=https://your-domain.com`, `HUGO_PORT=443`, and `HUGO_APPENDPORT=false`. Change `1313:1313` to `443:443` for the `ports` in `docker-compose.yml`.\n4. Spin up your Quartz container and enjoy it at `https://your-domain.com`!\n\n### To a Domain using a Reverse Proxy\n\nIf you want to serve more than just Quartz to the internet on this machine (or don't want to use the Cloudflare registrar and proxy), you should follow the steps in the section above (as appropriate) and also set up a reverse proxy, like [Traefik](https://doc.traefik.io/traefik). Be sure to configure your TLS certificates too!\n","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":["setup"]},"/quartz-docs/editing":{"title":"Editing Content in Quartz","content":"\n## Editing \nQuartz runs on top of [Hugo](https://gohugo.io/) so all notes are written in [Markdown](https://www.markdownguide.org/getting-started/).\n\n### Folder Structure\nHere's a rough overview of what's what.\n\n**All content in your garden can found in the `/content` folder.** To make edits, you can open any of the files and make changes directly and save it. You can organize content into any folder you'd like.\n\n**To edit the main home page, open `/content/_index.md`.**\n\n### Front Matter\nHugo is picky when it comes to metadata for files. Make sure that your title is double-quoted and that you have a title defined at the top of your file like so, otherwise the generated page will not have a title!\n\nYou can also add tags here as well.\n\n```yaml\n---\ntitle: \"Example Title\"\ntags:\n- example-tag\n---\n\nRest of your content here...\n```\n\n### Obsidian\nI recommend using [Obsidian](http://obsidian.md/) as a way to edit and grow your digital garden. It comes with a really nice editor and graphical interface to preview all of your local files.\n\nThis step is **highly recommended**.\n\n\u003e 🔗 Step 3: [How to setup your Obsidian Vault to work with Quartz](quartz-docs/obsidian.md)\n\n## Previewing Changes\nThis step is purely optional and mostly for those who want to see the published version of their digital garden locally before opening it up to the internet. This is *highly recommended* but not required.\n\n\u003e 👀 Step 4: [Preview Quartz Changes](quartz-docs/preview%20changes.md)\n\nFor those who like to live life more on the edge, viewing the garden through Obsidian gets you pretty close to the real thing.\n\n## Publishing Changes\nNow that you know the basics of managing your digital garden using Quartz, you can publish it to the internet!\n\n\u003e 🌍 Step 5: [Hosting Quartz online!](quartz-docs/hosting.md)\n\nHaving problems? Checkout our [FAQ and Troubleshooting guide](quartz-docs/troubleshooting.md).\n","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":["setup"]},"/quartz-docs/hosting":{"title":"Deploying Quartz to the Web","content":"\n## Hosting on GitHub Pages\nQuartz is designed to be effortless to deploy. If you forked and cloned Quartz directly from the repository, everything should already be good to go! Follow the steps below.\n\n### Enable GitHub Actions Permissions\nBy default, GitHub disables workflows from modifying your files (for good reason!). However, Quartz needs this to write the actual site files back to GitHub.\n\nHead to `Settings \u003e Action \u003e General \u003e Workflow Permissions` and choose `Read and Write Permissions`\n\n![[quartz-docs/images/github-actions.png]]\n*Enable GitHub Actions*\n\n### Enable GitHub Pages\n\nHead to the 'Settings' tab of your forked repository and go to the 'Pages' tab.\n\n1. (IMPORTANT) Set the source to deploy from `master` (and not `hugo`) using `/ (root)`\n2. Set a custom domain here if you have one!\n\n![Enable GitHub Pages](quartz-docs/images/github-pages.png)*Enable GitHub Pages*\n\n### Pushing Changes\nTo see your changes on the internet, we need to push it them to GitHub. Quartz is a `git` repository so updating it is the same workflow as you would follow as if it were just a regular software project.\n\n```shell\n# Navigate to Quartz folder\ncd \u003cpath-to-quartz\u003e\n\n# Commit all changes\ngit add .\ngit commit -m \"message describing changes\"\n\n# Push to GitHub to update site\ngit push origin hugo\n```\n\nNote: we specifically push to the `hugo` branch here. Our GitHub action automatically runs everytime a push to is detected to that branch and then updates the `master` branch for redeployment.\n\n### Setting up the Site\nNow let's get this site up and running. Never hosted a site before? No problem. Have a fancy custom domain you already own or want to subdomain your Quartz? That's easy too.\n\nHere, we take advantage of GitHub's free page hosting to deploy our site. Change `baseURL` in `/config.toml`. \n\nMake sure that your `baseURL` has a trailing `/`!\n\n[Reference `config.toml` here](https://github.com/jackyzha0/quartz/blob/hugo/config.toml)\n\n```toml\nbaseURL = \"https://\u003cYOUR-DOMAIN\u003e/\"\n```\n\nIf you are using this under a subdomain (e.g. `\u003cYOUR-GITHUB-USERNAME\u003e.github.io/quartz`), include the trailing `/`. **You need to do this especially if you are using GitHub!**\n\n```toml\nbaseURL = \"https://\u003cYOUR-GITHUB-USERNAME\u003e.github.io/quartz/\"\n```\n\nChange `cname` in `/.github/workflows/deploy.yaml`. Again, if you don't have a custom domain to use, you can use `\u003cYOUR-USERNAME\u003e.github.io`.\n\nPlease note that the `cname` field should *not* have any path `e.g. end with /quartz` or have a trailing `/`.\n\n[Reference `deploy.yaml` here](https://github.com/jackyzha0/quartz/blob/hugo/.github/workflows/deploy.yaml)\n\n```yaml {title=\".github/workflows/deploy.yaml\"}\n- name: Deploy  \n  uses: peaceiris/actions-gh-pages@v3  \n  with:  \n\tgithub_token: ${{ secrets.GITHUB_TOKEN }} # this can stay as is, GitHub fills this in for us!\n\tpublish_dir: ./public  \n\tpublish_branch: master\n\tcname: \u003cYOUR-DOMAIN\u003e\n```\n\nHave a custom domain? [Learn how to set it up with Quartz ](quartz-docs/custom%20Domain.md).\n\n### Ignoring Files\nOnly want to publish a subset of all of your notes? Don't worry, Quartz makes this a simple two-step process.\n\n❌ [Excluding pages from being published](quartz-docs/ignore%20notes.md)\n\n## Docker Support\nIf you don't want to use a hosting service, you can host using [Docker](quartz-docs/docker.md) instead!\nI would *not use this method* unless you know what you are doing.\n\n---\n\nNow that your Quartz is live, let's figure out how to make Quartz really *yours*!\n\n\u003e Step 6: 🎨 [Customizing Quartz](quartz-docs/config.md)\n\nHaving problems? Checkout our [FAQ and Troubleshooting guide](quartz-docs/troubleshooting.md).\n","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":["setup"]},"/quartz-docs/ignore-notes":{"title":"Ignoring Notes","content":"\n### Quartz Ignore\nEdit `ignoreFiles` in `config.toml` to include paths you'd like to exclude from being rendered.\n\n```toml\n...\nignoreFiles = [  \n    \"/content/templates/*\",  \n    \"/content/private/*\", \n    \"\u003cyour path here\u003e\"\n]\n```\n\n`ignoreFiles` supports the use of Regular Expressions (RegEx) so you can ignore patterns as well (e.g. ignoring all `.png`s by doing `\\\\.png$`).\nTo ignore a specific file, you can also add the tag `draft: true` to the frontmatter of a note.\n\n```markdown\n---\ntitle: Some Private Note\ndraft: true\n---\n...\n```\n\nMore details in [Hugo's documentation](https://gohugo.io/getting-started/configuration/#ignore-content-and-data-files-when-rendering).\n\n### Global Ignore\nHowever, just adding to the `ignoreFiles` will only prevent the page from being access through Quartz. If you want to prevent the file from being pushed to GitHub (for example if you have a public repository), you need to also add the path to the `.gitignore` file at the root of the repository.","lastmodified":"2023-10-12T15:43:15.227097535Z","tags":[]},"/quartz-docs/obsidian":{"title":"Obsidian Vault Integration","content":"\n## Setup\nObsidian is the preferred way to use Quartz. You can either create a new Obsidian Vault or link one that your already have.\n\n### New Vault\nIf you don't have an existing Vault, [download Obsidian](https://obsidian.md/) and create a new Vault in the `/content` folder that you created and cloned during the [setup](quartz-docs/setup.md) step.\n\n### Linking an existing Vault\nThe easiest way to use an existing Vault is to copy all of your files (directory and hierarchies intact) into the `/content` folder.\n\n## Settings\nGreat, now that you have your Obsidian linked to your Quartz, let's fix some settings so that they play well.\n\nOpen Settings \u003e Files \u0026 Links and look for these two items:\n\n1. Set the **New link format** to **Absolute Path in vault**. If you have a completely flat vault (no folders), this step isn't necessary.\n2. Turn **on** the **Automatically update internal links** setting.\n\n\n![[quartz-docs/images/obsidian-settings.png]]*Obsidian Settings*\n\n## Templates\nInserting front matter everytime you want to create a new Note gets annoying really quickly. Luckily, Obsidian supports templates which makes inserting new content really easily.\n\n\u003e [!WARNING]\n\u003e \n\u003e **If you decide to overwrite the `/content` folder completely, don't remove the `/content/templates` folder!**\n\nHead over to Options \u003e Core Plugins and enable the Templates plugin. Then go to Options \u003e Hotkeys and set a hotkey for 'Insert Template' (I recommend `[cmd]+T`). That way, when you create a new note, you can just press the hotkey for a new template and be ready to go!\n\n\u003e 👀 Step 4: [Preview Quartz Changes](quartz-docs/preview%20changes.md)\n","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":["setup"]},"/quartz-docs/philosophy":{"title":"Quartz Philosophy","content":"\n\u003e “[One] who works with the door open gets all kinds of interruptions, but [they] also occasionally gets clues as to what the world is and what might be important.” — Richard Hamming\n\n## Why Quartz?\nHosting a public digital garden isn't easy. There are an overwhelming number of tutorials, resources, and guides for tools like [Notion](https://www.notion.so/), [Roam](https://roamresearch.com/), and [Obsidian](https://obsidian.md/), yet none of them have super easy to use *free* tools to publish that garden to the world.\n\nI've personally found that\n1. It's nice to access notes from anywhere\n2. Having a public digital garden invites open conversations\n3. It makes keeping personal notes and knowledge *playful and fun*\n\nI was really inspired by [Bianca](https://garden.bianca.digital/) and [Joel](https://joelhooks.com/digital-garden)'s digital gardens and wanted to try making my own.\n\n**The goal of Quartz is to make hosting your own public digital garden free and simple.** You don't even need your own website. Quartz does all of that for you and gives your own little corner of the internet.\n","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/quartz-docs/preview-changes":{"title":"Preview Changes","content":"\nIf you'd like to preview what your Quartz site looks like before deploying it to the internet, the following\ninstructions guide you through installing the proper dependencies to run it locally.\n\n\n## Install `hugo-obsidian`\nThis step will generate the list of backlinks for Hugo to parse. Ensure you have [Go](https://golang.org/doc/install) (\u003e= 1.16) installed.\n\n```bash\n# Install and link `hugo-obsidian` locally\ngo install github.com/jackyzha0/hugo-obsidian@latest\n```\n\nIf you are running into an error saying that `command not found: hugo-obsidian`, make sure you set your `GOPATH` correctly (see [[quartz-docs/troubleshooting#`command not found: hugo-obsidian`|the troubleshooting page]])! This will allow your terminal to correctly recognize hugo-obsidian as an executable.\n\n##  Installing Hugo\nHugo is the static site generator that powers Quartz. [Install Hugo with \"extended\" Sass/SCSS version](https://gohugo.io/getting-started/installing/) first. Then,\n\n```bash\n# Navigate to your local Quartz folder\ncd \u003clocation-of-your-local-quartz\u003e\n\n# Start local server\nmake serve\n\n# View your site in a browser at http://localhost:1313/\n```\n\n\u003e [!INFO] Docker Support\n\u003e\n\u003e If you have the Docker CLI installed already, you can avoid installing `hugo-obsidian` and `hugo`. Instead, open your terminal, navigate to your folder with Quartz and run `make docker`\n\nAfterwards, start the Hugo server as shown above and your local backlinks and interactive graph should be populated! Now, let's get it hosted online.\n\n\u003e 🌍 Step 5: [Hosting Quartz online!](quartz-docs/hosting.md)\n","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":["setup"]},"/quartz-docs/search":{"title":"Search","content":"\nQuartz supports two modes of searching through content.\n\n## Full-text\nFull-text search is the default in Quartz. It produces results that *exactly* match the search query. This is easier to setup but usually produces lower quality matches.\n\n```yaml {title=\"data/config.yaml\"}\n# the default option\nenableSemanticSearch: false\n```\n\n## Natural Language\nNatural language search is powered by [Operand](https://beta.operand.ai/). It understands language like a person does and finds results that best match user intent. In this sense, it is closer to how Google Search works.\n\nNatural language search tends to produce higher quality results than full-text search.\n\nHere's how to set it up.\n\n1. Login or Register for a new Operand account. Click the verification link sent to your email, and you'll be redirected to the dashboard. (Note) You do not need to enter a credit card to create an account, or get started with the Operand API. The first $10 of usage each month is free. To learn more, see pricing. If you go over your free quota, we'll (politely) reach out and ask you to configure billing.\n2. Create your first index. On the dashboard, under \"Indexes\", enter the name and description of your index, and click \"Create Index\". Note down the ID of the index (obtained by clicking on the index name in the list of indexes), as you'll need it in the next step. IDs are unique to each index, and look something like `uqv1duxxbdxu`.\n3. Click into the index you've created. Under \"Index Something\", select \"SITEMAP\" from the dropdown and click \"Add Source\".\n4. For the \"Sitemap.xml URL\", put your deployed site's base URL followed by `sitemap.xml`. For example, for `quartz.jzhao.xyz`, put `https://quartz.jzhao.xyz/sitemap.xml`. Leave the URL Regex empty. \n5. Get your API key. On the dashboard, under \"API Keys\", you can manage your API keys. If you don't already have an API key, click \"Create API Key\". You'll need this for the next step.\n6. Open `data/config.yaml`. Set `enableSemanticSearch` to `true`, `operandApiKey` to your copied key, and `operandIndexId` to the ID of the index we created from earlier..\n\n```yaml {title=\"data/config.yaml\"}\n# the default option\nsearch:\n  enableSemanticSearch: true\n  operandApiKey: \"jp9k5hudse2a828z98kxd6z3payi8u90rnjf\"\n  operandIndexId: \"s0kf3bd6tldw\"\n```\n7. Push your changes to the site and wait for it to deploy.\n8. Check the Operand dashboard and wait for your site to index. Enjoy natural language search powered by Operand!\n","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/quartz-docs/setup":{"title":"Setup","content":"\n## Making your own Quartz\nSetting up Quartz requires a basic understanding of `git`. If you are unfamiliar, [this resource](https://resources.nwplus.io/2-beginner/how-to-git-github.html) is a great place to start!\n\n### Forking\n\u003e A fork is a copy of a repository. Forking a repository allows you to freely experiment with changes without affecting the original project.\n\nNavigate to the GitHub repository for the Quartz project:\n\n📁 [Quartz Repository](https://github.com/jackyzha0/quartz)\n\nThen, Fork the repository into your own GitHub account. **Make sure that when you fork, you _uncheck_ the 'Copy the `hugo` branch only' option**.\n\nIf you don't have an account, you can make on for free [here](https://github.com/join). More details about forking a repo can be found on [GitHub's documentation](https://docs.github.com/en/get-started/quickstart/fork-a-repo).\n\n![[quartz-docs/images/fork.png]]\n\n### Cloning\nAfter you've made a fork of the repository, you need to download the files locally onto your machine. Ensure you have `git`, then type the following command in your terminal replacing `YOUR-USERNAME` with your GitHub username.\n\n```shell\ngit clone https://github.com/YOUR-USERNAME/quartz\n```\n\n## Editing\nGreat! Now you have everything you need to start editing and growing your digital garden. If you're ready to start writing content already, check out the recommended flow for editing notes in Quartz.\n\n\u003e ✏️ Step 2: [Editing Notes in Quartz](quartz-docs/editing.md)\n\nHaving problems? Checkout our [FAQ and Troubleshooting guide](quartz-docs/troubleshooting.md).\n","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":["setup"]},"/quartz-docs/showcase":{"title":"Showcase","content":"\nWant to see what Quartz can do? Here are some cool community gardens :)\n\n- [Quartz Documentation (this site!)](https://quartz.jzhao.xyz/)\n- [Jacky Zhao's Garden](https://jzhao.xyz/)\n- [Scaling Synthesis - A hypertext research notebook](https://scalingsynthesis.com/)\n- [AWAGMI Intern Notes](https://notes.awagmi.xyz/)\n- [Shihyu's PKM](https://shihyuho.github.io/pkm/)\n- [SlRvb's Site](https://slrvb.github.io/Site/)\n- [Course notes for Information Technology Advanced Theory](https://a2itnotes.github.io/quartz/)\n- [Brandon Boswell's Garden](https://brandonkboswell.com)\n- [Siyang's Courtyard](https://siyangsun.github.io/courtyard/)\n- [Data Dictionary 🧠](https://glossary.airbyte.com/)\n- [sspaeti.com's Second Brain](https://brain.sspaeti.com/)\n- [oldwinterの数字花园](https://garden.oldwinter.top/)\n- [SethMB Work](https://sethmb.xyz/)\n- [Abhijeet's Math Wiki](https://abhmul.github.io/quartz/Math-Wiki/)\n- [Mike's AI Garden 🤖🪴](https://mwalton.me/)\n\nIf you want to see your own on here, submit a [Pull Request adding yourself to this file](https://github.com/jackyzha0/quartz/blob/hugo/content/notes/showcase.md)!\n","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/quartz-docs/troubleshooting":{"title":"Troubleshooting and FAQ","content":"\nStill having trouble? Here are a list of common questions and problems people encounter when installing Quartz.\n\nWhile you're here, join our [Discord](https://discord.gg/cRFFHYye7t) :)\n\n### Does Quartz have Latex support?\nYes! See [CJK + Latex Support (测试)](quartz-docs/CJK%20+%20Latex%20Support%20(测试).md) for a brief demo.\n\n### Can I use \\\u003cObsidian Plugin\\\u003e in Quartz?\nUnless it produces direct Markdown output in the file, no. There currently is no way to bundle plugin code with Quartz.\n\nThe easiest way would be to add your own HTML partial that supports the functionality you are looking for.\n\n### My GitHub pages is just showing the README and not Quartz\nMake sure you set the source to deploy from `master` (and not `hugo`) using `/ (root)`! See more in the [hosting](quartz-docs/hosting.md) guide\n\n### Some of my pages have 'January 1, 0001' as the last modified date\nThis is a problem caused by `git` treating files as case-insensitive by default and some of your posts probably have capitalized file names. You can turn this off in your Quartz by running this command.\n\n```shell\n# in the root of your Quartz (same folder as config.toml)\ngit config core.ignorecase true\n\n# or globally (not recommended)\ngit config --global core.ignorecase true\n```\n\n### Can I publish only a subset of my pages?\nYes! Quartz makes selective publishing really easy. Heres a guide on [excluding pages from being published](quartz-docs/ignore%20notes.md).\n\n### Can I host this myself and not on GitHub Pages?\nYes! All built files can be found under `/public` in the `master` branch. More details under [hosting](quartz-docs/hosting.md).\n\n### `command not found: hugo-obsidian`\nMake sure you set your `GOPATH` correctly! This will allow your terminal to correctly recognize `hugo-obsidian` as an executable.\n\n```shell\n# Add the following 2 lines to your ~/.bash_profile (~/.zshrc if you are on Mac)\nexport GOPATH=/Users/$USER/go\nexport PATH=$GOPATH/bin:$PATH\n\n# In your current terminal, to reload the session\nsource ~/.bash_profile # again, (~/.zshrc if you are on Mac)\n```\n\n### How come my notes aren't being rendered?\nYou probably forgot to include front matter in your Markdown files. You can either setup [Obsidian](quartz-docs/obsidian.md) to do this for you or you need to manually define it. More details in [the 'how to edit' guide](quartz-docs/editing.md).\n\n### My custom domain isn't working!\nWalk through the steps in [the hosting guide](quartz-docs/hosting.md) again. Make sure you wait 30 min to 1 hour for changes to take effect.\n\n### How do I setup analytics?\nQuartz by default uses [Plausible](https://plausible.io/) for analytics. \n\nIf you would prefer to use Google Analytics, you can follow this [guide in the Hugo documentation](https://gohugo.io/templates/internal/#google-analytics). \n\nAlternatively, you can also import your Google Analytics data into Plausible by [following this guide](https://plausible.io/docs/google-analytics-import).\n\n\n### How do I change the content on the home page?\nTo edit the main home page, open `/content/_index.md`.\n\n### How do I change the colours?\nYou can change the theme by editing `assets/custom.scss`. More details on customization and themeing can be found in the [customization guide](quartz-docs/config.md).\n\n### How do I add images?\nYou can put images anywhere in the `/content` folder.\n\n```markdown\nExample image (source is in content/notes/images/example.png)\n![Example Image](/content/notes/images/example.png)\n```\n\n### My Interactive Graph and Backlinks aren't up to date\nBy default, the `linkIndex.json` (which Quartz needs to generate the Interactive Graph and Backlinks) are not regenerated locally. To set that up, see the guide on [local editing](quartz-docs/editing.md)\n\n### Can I use React/Vue/some other framework?\nNot out of the box. You could probably make it work by editing `/layouts/_default/single.html` but that's not what Quartz is designed to work with. 99% of things you are trying to do with those frameworks you can accomplish perfectly fine using just vanilla HTML/CSS/JS.\n\n## Still Stuck?\nQuartz isn't perfect! If you're still having troubles, file an issue in the GitHub repo with as much information as you can reasonably provide. Alternatively, you can message me on [Twitter](https://twitter.com/_jzhao) and I'll try to get back to you as soon as I can.\n\n🐛 [Submit an Issue](https://github.com/jackyzha0/quartz/issues)\n","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/quartz-docs/updating":{"title":"Updating","content":"\nHaven't updated Quartz in a while and want all the cool new optimizations? On Unix/Mac systems you can run the following command for a one-line update! This command will show you a log summary of all commits since you last updated, press `q` to acknowledge this. Then, it will show you each change in turn and press `y` to accept the patch or `n` to reject it. Usually you should press `y` for most of these unless it conflicts with existing changes you've made! \n\n```shell\nmake update\n```\n\nOr, if you don't want the interactive parts and just want to force update your local garden (this assumed that you are okay with some of your personalizations been overriden!)\n\n```shell\nmake update-force\n```\n\nOr, manually checkout the changes yourself.\n\n\u003e [!warning] Warning!\n\u003e\n\u003e If you customized the files in `data/`, or anything inside `layouts/`, your customization may be overwritten!\n\u003e Make sure you have a copy of these changes if you don't want to lose them.\n\n\n```shell\n# add Quartz as a remote host\ngit remote add upstream git@github.com:jackyzha0/quartz.git\n\n# index and fetch changes\ngit fetch upstream\ngit checkout -p upstream/hugo -- layouts .github Makefile assets/js assets/styles/base.scss assets/styles/darkmode.scss config.toml data \n```\n","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/react-integration":{"title":"react integration","content":"#doc\n\ncr-sqlite is integrated to React via `useQuery` hooks. [docs](https://vlcn.io/docs/js/react)\n\n# Main considerations:\n- opening IndexedDB transactions is expensive. Hence we must gather all `useQuery` requests in a given render pass and issue them in a single transaction\n- `useQuery` must be reactive. We use the `tables_used` vtab to extract tables hit by a query to enable re-running them. We could get smarter and do an [[inverted database]] and even [[incremental updates to cached queries]] to enable [[fully synchronous reactivity]].\n- [[waterfalling \u0026 flickering]]\n- The [[wasm tax]] looks pretty high. Maybe it is better to gather small queries into a large query rather than doing many tiny queries. This could change when [[Moving to OPFS]] as in that situation we're going over a worker boundary but we can batch returns into the same tick.\n\n# Other Improvements\n- Have the [[DB cache]] live longer than a single pass through the event loop? Maybe not given we cache in the reactive layer anyhow.\n\n# Future\n- [[Moving to OPFS]] will require a re-work of the react integration. We can presumably batch all returned data into the same event loop tick in this situation, however, which may actually be a beneift\n\n# Structured Data\n- Drizzle allows us to pull [[converting relations to hierarchies|hierarchical data]] rather easily: https://orm.drizzle.team/docs/rqb\n\t- We could use that to\n\t\t- Enable larger queries to deal with the [[wasm tax]]\n\t\t- Enable the GraphQL like vision https://twitter.com/schickling/status/1599076832107630594\n\t\t- Bring the [vanilla fetch](https://github.com/tantaman/vanilla-fetch) idea to Vulcan","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/roadmap":{"title":"roadmap","content":"The goal is to create the embedded database of the next generation. Embedded databases for application development have not kept up with the changing landscape of:\n\n1. Device proliferation\n2. Collaboration\n3. Reactive programming\n\n((vlcn business strategy))","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/row-level-security":{"title":"row level security","content":"\nHow might we implement row level security for the CRDT database?\n\nThe big issue is that when privacy changes it can cause a bunch of new data to be visible that never had a write made to it. Given our [[incrementalism]] is based on \"stuff being written\", we need to somehow identify that this new data (which didn't receive a write) is available for the user. ^d9509b\n\nThink of it like this:\n![[user promoted to god example]]\n\nMy latest thinking is around using [[bloom filters for row level security]] + [[client defined query based sync]].\n\n[[why not encryption for permissions]]","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/rx-cache-design":{"title":"rx cache design","content":"In other vault.","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/server-overrides":{"title":"server overrides","content":"\n#project\n\n\"server overrides\" or [[server authoritative]] data is the means to allow the server to overrule clients.\n\nUsers can implement this in user-space by processing the changeset log before applying it and throwing out changes they do not agree with. Of course the client passing bad changes will be diverged at this point.\n\nSo if they throw out changes, they must tell the client to roll back the given rows or apply a given state for those rows.\n\nRoll-back:\n1. Send the server versions of those rows that the client must take and apply\n\t1. Apply these outside the merge path? Or make a \"override\" path that applies them without bumping any versions? \n\t2. Or bump the server versions where server did an override? dos problems?\n\t3. Server responds with \"overrule\" with cols and version to roll back to? And the client's db version that was overruled?","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/server-side-sqlite":{"title":"server side sqlite","content":"\n\u003e I'd be curious to hear how your experience with Turso \u0026 D1 is. Turso looks interesting since it's a fully managed solution but IIRC it doesn't really do interactive transactions. It also seems like it suffers from the N+1 query issue since the application and data aren't colocated. I looked at D1 briefly when they first came out but I haven't taken a lot since they did their rewrite.\n\u003e \n\u003e Let me know if you have any other questions or if I can help out at all.\n\nI should turn this into a blog post but if you're willing to bear with me...\n# Turso\n\nTurso has the simplest experience in terms of being \"push-button\" ready.\n\nThe big drawback (which you allude to), however, is that Turso is setting up SQLite much like a traditional DB. You interact with it over HTTP rather than it being embedded in-process, throwing out many of the advantages of SQLite. Turso is working on \"embedded replicas\" which'll let you run SQLite in-process again. I believe a preview of this just landed. This should also fix the interactive transaction story.\n\nThe other drawback to Turso is that you can't programmatically spin up new DBs. E.g., if you do  want to create a new DB each time a user is registered or does something. That's currently impossible but also something they're working on fixing.\n\nA big bonus of Turso, besides being simple, is that they're improving the write concurrency story of SQLite. I think based on Hekaton?\n\nThe last drawback of Turso is config \u0026 extensions. You can't just treat it exactly like SQLite and load whatever extensions you want. If you need extensions added, they currently must be on the turso allow list and built into their release. Probably doesn't matter to most people.\n\nSmall drawback -- Turso requires you to use custom bindings rather than any old SQLite bindings you like.\n\n# D1\n\nI'm not a fan of D1 at all. I've only had it described to me by one of their VPs but my understanding is:\n\n1. DB instances are currently tied to a single \"durable object\"\n2. DBs cannot be shared between durable objects\n3. You can't use whatever SQLite bindings you want and you have to use their custom wrapper APIs\n4. Extensions can't be loaded into D1. Loading a SQLite extension requires adding it to the global D1 build for all D1 users.\n\nI can't come up with any positives for D1... It feels like a product solving all the wrong problems to me.\n\n# LiteFS \u0026 Fly\n\nLiteFS is my personal favorite.\n\nPros:\n1. Can use any SQLite bindings and (I think?) any SQLite3 version \n2. Can load arbitrary extensions\n3. DB is embedded into the application process as expected\n4. Read replicas are simple to set up\n\n\nThe things preventing fly from knocking it out of the park:\n1. Getting set up with LiteFS is a lot of configuration work rather than being as simple as Turso.\n2. Write forwarding is not transparent and, if you can't use the litefs proxy, takes some effort to get working. Ideally I could use my DB as normal and write transactions would automatically forward.\n\nAn unknown is if SQLite's write concurrency is good enough for use on the server side and LiteFS makes no improvements here.\n\n\n","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/shared-access-handle-pool":{"title":"shared access handle pool","content":"official sqlite impl: https://sqlite.org/forum/forumpost/dc2f39cd5cf1bfc4\nspeed tests: https://sqlite.org/forum/forumpost/1d767118bc077703","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/signia":{"title":"signia","content":"\nhttps://twitter.com/jitl/status/1689656021264855040\n\n- signals on query results?\n- diff query results?\n- ","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/sqlite-speed-tests":{"title":"sqlite speed tests","content":"https://sqlite.org/forum/forumpost/1d767118bc077703","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/streaming-reactivity":{"title":"streaming reactivity","content":"\nFrom Jerome (https://discord.com/channels/989870439897653248/989870440585494530/1128026067819561084):\n\nbasically, to support _any_ queries (even very complex ones), we have to check if anything was upserted or deleted on every change. even if the change is \"inserting a new row\", it's possible a negative constraint in the query removes a row from the result set.\n\nso what I'm doing is:\n- modify the query to prepend all primary keys for all tables mentioned in the query, we also re-alias all selected columns like: `col_0`, `col_1`, ... (you'll see why later)\n- store the query results in a temporary table via `INSERT INTO ... SELECT ...` (this is possibly problematic since it goes away with the connection and I'm using a connection pool)\n- the temporary table has an extra column called `__corro_rowid` which is basically the position of the resulting row. it's an auto-incrementing integer\n- we stream back the rows with their `__corro_rowid`, but not the extra primary keys for all tables we added as columns. effectively the client gets a stream of `{rowid: i64, cells: Vec\u003cSqliteValue\u003e}`\n- now, when there's a change (any change), we have to prepare 2 queries, which is a bit of bummer...\n- the first query is to detect any upsertions: \n```sql\nINSERT INTO \u003ctemp-table\u003e (__corro_pk_table1_pk1, ..., col_0, ...)\n  SELECT * FROM (\u003coriginal query w/ added where clause to only fetch rows related to the current change\u003e EXCEPT SELECT ... FROM \u003ctempl-table\u003e) WHERE 1\n  ON CONFLICT (__corro_pk_table1_pk1, \u003clist of primary key columns\u003e)\n    DO UPDATE SET\n       col_0 = excluded.col_0, ...\nRETURNING *\n```\n- the second query is to detect any deletes: \n```sql\nDELETE FROM \u003ctemp-table\u003e\n  WHERE (__corro_pk_table1_pk1, \u003clist of primary key columns\u003e) IN (SELECT __corro_pk_table1_pk1, \u003clist of primary key columns\u003e FROM \u003ctemp-table\u003e EXCEPT \u003coriginal query w/ added where clause to only fetch rows related to the current change\u003e)) RETURNING *\n```","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/sync-is-hard":{"title":"sync is hard","content":"\nSync that allows:\n- offline editing\n- realtime collaboration\n\nIs harder than it should be. The infra to make it possible _is just missing_. Example case studies:\n\n- https://www.youtube.com/watch?v=Wo2m3jaJixU\n- https://madebyevan.com/algos/\n- https://www.figma.com/blog/how-figmas-multiplayer-technology-works/\n- https://blog.superhuman.com/architecting-a-web-app-to-just-work-offline-part-1/\n- https://softwareengineeringdaily.com/2020/03/31/facebook-messenger-engineering-with-mohsen-agsen/\n- https://blog.danlew.net/2017/02/14/airplane-mode-enabling-trello-mobile-offline/#:~:text=Good%20news%2C%20everyone!,re%20flying%20on%20a%20plane.","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/sync-server-plans":{"title":"sync server plans","content":"\nStarting from our websocket sync server -- https://github.com/vlcn-io/cr-sqlite/pull/316\n\n- Create native inbound and outbound stream abstractions as documented in https://github.com/vlcn-io/cr-sqlite/issues/38\n- create server + rest api for strongly consistent stuff\n\t- expose rest API as vtab?\n\t- rest api ","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/tables-used":{"title":"tables used","content":"\nSQLite `get_tables_used` is very slow. We need to calculate this statically or cache all prepared versions of `tables_used` and the result(s).","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/theoretical-sqlite-wasm-speed":{"title":"theoretical sqlite wasm speed","content":"\nhttps://gist.github.com/tantaman/28f1fc4cb7ccc0012bf3bd8ccca76eed\n\n- [[tables used]] is deathly slow.\n- preparing queries is slow.\n- thread\n\nTODO:\n- with OPFS enabled -- how long those same tests take\n- with Roy's lock_mode_exclusive + shared access handle approach\n\t- can we WAL mode this?\n- \n","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/tinybase":{"title":"tinybase","content":"\n#project\n\n- Needs to only ingest synced row on sync\n- Needs to only saved modified rows on persist\n\n- Loads just the changed row(s) after sync events?\n- Saves just the changed state to DB on interval?\n- Paging into memory:\n\t- Specify a query to load? Regular OO model but with queries specified for connections?\n\t- TinyBase query system that maps to SQL queries if needed?","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/ts-ast":{"title":"ts ast","content":"\nView the TS AST: https://ts-ast-viewer.com/#\nand: https://ts-creator.js.org/","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/ts-transform-plugin":{"title":"ts transform plugin","content":"\nOfficial support for TS Transformers look dubious:\nhttps://github.com/microsoft/TypeScript/issues/14419\nhttps://github.com/microsoft/TypeScript/issues/16607\nhttps://github.com/microsoft/TypeScript/issues/54276\n\nMight be better to do as an ESLint plugin which is stable.\n\nHandbook on writing a transformer: https://github.com/itsdouges/typescript-transformer-handbook\n\nhttps://dev.doctorevidence.com/how-to-write-a-typescript-transform-plugin-fc5308fdd943\n\nTransformers are not officially supported in the build step of TypeScript.\n\nBut that should be a non issue? We can run our transformer before compilation? Well.. we need vite integration. Vite would need to run the transforms.\n\nWe can watch the TS files and run them. But if someone has many to run? They need to be pipelined.\n\n---\n\n`ts-patch`: https://github.com/nonara/ts-patch\nreplace the vite typescript compiler? https://github.com/vitejs/vite/discussions/7580\n\nor `ts-patch install` to overwrite local node_modules typescript.","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/type-safe-sql":{"title":"type safe sql","content":"\n#project\n\nPeople keep writing crappy query builders in order to get type safety in their language of choice.\n\nWe can get it by parsing:\n1. The schema file(s)\n2. The SQL queries themselves\n\nUsage looks like:\n\n```ts\nconst schema = ingest(schema_files...);\nconst q = schema.sql\u003c_autoinject from eslint_\u003e`SELECT * FROM x join y ...`;\n```\n\n`q` should have full type information about what was selected and the types of those things. As a branded type?\nIdeally we can do inline autocomplete in the SQL string. Do GraphQL plugins do this in GraphQL frags that we can get implementation tips from?\n\n# V0\n1. Walk the AST https://github.com/itsdouges/typescript-transformer-handbook , https://github.com/microsoft/TypeScript/wiki/Using-the-Compiler-API\n\t1. find\n\t   ```ts\n\t   SCHEMA.sql\u003cNAME\u003e`sql`\n\t   ```\n2. Resolve SCHEMA to the thing imported and applied (js file with content property)\n3. Used that as schema type source\n4. Generate types from tagged literal contents\n5. Output to typing file in `__generated__` dir as sibling to current file named NAME.\n\n# V1\nAn initial bare-bones version:\n1. Syntax highlighting provided by some other pkg\n\t1. e.g, https://github.com/thebearingedge/vscode-sql-lit\n2. Generate types via your own codegen, stick them into after tag `\u003c\u003e` via eslint.\n\t1. Can write this step in Rust? Given you'll need the parser for RX anyway.\n3. Completions come later\n\nEven simpler start?\n```ts\n// Watch SQL files and generate Schema.d.ts files of the same name?\n// Then they can be imported and used.\n// Ts TS Plugin (or ESLint plugin or whatever..) will scan for their usage\n// so we can generate types\nconst q = schema.sql\u003cUserType\u003e`...`\n```\n\nCan we do TS Source Transform rather than ESLint?\nhttps://github.com/Microsoft/TypeScript/issues/29993\n[[ts ast]]\n[[ts transform plugin]]\n\n---\n\n# Inspiration\nhttps://github.com/Quramy/ts-graphql-plugin\nts-graphpl-plugin config:\n\n```json\n{\n  \"compilerOptions\": {\n    \"module\": \"commonjs\",\n    \"target\": \"es5\",\n    \"plugins\": [\n      {\n        \"name\": \"ts-graphql-plugin\",\n        \"schema\": \"path-or-url-to-your-schema.graphql\",\n        \"tag\": \"gql\"\n      }\n    ]\n  }\n}\n```\n\nWe can take:\n- sql files\n- or js files with a sql member exported?\n\t- `import * as foo from 'thing';` and access the configured property?\n\nCLI commands, bundler integrations.\n\n# Implementation References\n- LSP Plugin Reference: https://github.com/microsoft/TypeScript/wiki/Writing-a-Language-Service-Plugin\n- https://github.com/Microsoft/typescript-template-language-service-decorator - high level / the decorator framework for tagged literals\n- https://github.com/mjbvz/vscode-lit-html\n- https://github.com/Microsoft/vscode/issues/41113 - high level overview\n- https://github.com/joe-re/sql-language-server - abandoned LSP for SQL. Requires a SQLite db file :/ rather than just taking schema file(s) as input\n- https://github.com/codeschool/sqlite-parser - sqlite -\u003e AST\n- https://github.com/styled-components/typescript-styled-plugin - recent impl\n\n# References\n- https://discord.com/channels/989870439897653248/989870440585494530/1125443906675409010\n- [[TreeQL]] - https://github.com/tantaman/composed-sql","lastmodified":"2023-10-12T15:43:15.231097772Z","tags":[]},"/user-promoted-to-god-example":{"title":"user promoted to god example","content":"\n- A user is added to the system.\n- The user can only see their data\n- The user is upgraded to admin\n- The user can no see _all data_ on the site.","lastmodified":"2023-10-12T15:43:15.235098009Z","tags":[]},"/vanilla-fetch":{"title":"vanilla fetch","content":"#project \n\nUnlock [this pattern](https://github.com/tantaman/vanilla-fetch) with [[TreeQL]]","lastmodified":"2023-10-12T15:43:15.235098009Z","tags":[]},"/vscode-semantic-highlighting":{"title":"vscode semantic highlighting","content":"https://code.visualstudio.com/api/language-extensions/semantic-highlight-guide\n\nYes, you can indeed install a Visual Studio Code extension from a local directory. Here's how:\n\n1. First, make sure you have Node.js installed in your system as it's required for running the `vsce` package, a command line tool for packaging VS Code extensions. If you haven't, you can download it from [here](https://nodejs.org/).\n\n2. Next, install the `vsce` package globally using npm (which is included in Node.js) by running the following command in your terminal:\n\n   ```bash\n   npm install -g vsce\n   ```\n\n3. Once you have `vsce` installed, navigate to your extension's directory:\n\n   ```bash\n   cd path/to/your/extension\n   ```\n\n4. Then package your extension using `vsce`. This will create a `.vsix` file (VS Code's extension file format) in your extension's directory:\n\n   ```bash\n   vsce package\n   ```\n\n5. After the `.vsix` file is created, open VS Code and click on the extensions view icon on the Sidebar (or press `Ctrl+Shift+X`).\n\n6. Click on the `...` More Actions menu in the top-right corner of the Extensions view, and select `Install from VSIX...`.\n\n7. Locate the `.vsix` file in your filesystem and click `Install`.\n\nThat's it! You've installed your VS Code extension from a local directory. Now you should be able to test it and debug it locally. When you make changes, you'll have to repackage and reinstall the extension to test the new version.","lastmodified":"2023-10-12T15:43:15.235098009Z","tags":[]},"/weekly":{"title":"weekly","content":"- [[Sep 4-8 2023]]\n- [[Aug 28-1 2023]]\n- [[Aug 21-25 2023]]\n- [[Aug 14-18 2023]]\n- [[Aug 07-11 2023]]\n- [[July 31-04 2023]]\n- [[July 24-28, 2023]]\n- [[July 17-21, 2023]]\n- [[July 10-14, 2023]]\n- [[July 03-07, 2023]]\n- [[June 26-30, 2023]]\n\n","lastmodified":"2023-10-12T15:43:15.235098009Z","tags":[]},"/worker-coordinate":{"title":"worker coordinate","content":"#project \n\nWe can coordinate workers through service workers.\n```js\n\"use strict\";\n\n// Install the service worker as soon as possible.\nglobalThis.addEventListener('install', (/** @type {ExtendableEvent} */ event) =\u003e {\n  event.waitUntil(globalThis.skipWaiting());\n});\nglobalThis.addEventListener('activate', (/** @type {ExtendableEvent} */ event) =\u003e {\n  event.waitUntil(globalThis.clients.claim());\n});\n\n// Forward messages (and ports) from client to client.\nglobalThis.addEventListener('message', async event =\u003e {\n  if (event.data?.sharedService) {\n    const client = await globalThis.clients.get(event.data.clientId);\n    client.postMessage(event.data, event.ports);\n  }\n});\n\n// Tell clients their clientId. A service worker isn't actually needed\n// for a context to get its clientId, but this also doubles as a way\n// to verify that the service worker is active.\nglobalThis.addEventListener('fetch', async (/** @type {FetchEvent} */ event) =\u003e {\n  if (event.request.url === globalThis.registration.scope + 'clientId') {\n    return event.respondWith(new Response(event.clientId, {\n      headers: { \"Content-Type\": \"text/plain\" }\n    }));\n  }\n});\n```\n\n# References\nhttps://github.com/rhashimoto/wa-sqlite/discussions/81#discussioncomment-6223838\nhttps://github.com/rhashimoto/wa-sqlite/blob/master/demo/SharedService-sw/SharedService_ServiceWorker.js","lastmodified":"2023-10-12T15:43:15.235098009Z","tags":[]}}